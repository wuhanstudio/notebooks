{
  
    
        "post0": {
            "title": "Interpretable ML - COVID19",
            "content": "0. Load Data . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . np.set_printoptions(suppress=True) . covid = pd.read_csv(&quot;dataset/covid.csv&quot;) . print(&quot;AVG age for severity 0:&quot;, np.mean(covid[covid.Severity03 == 0].Age.to_numpy())) print(&quot;AVG age for severity 1:&quot;, np.mean(covid[covid.Severity03 == 1].Age.to_numpy())) print(&quot;AVG age for severity 2:&quot;, np.mean(covid[covid.Severity03 == 2].Age.to_numpy())) print(&quot;AVG age for severity 3:&quot;, np.mean(covid[covid.Severity03 == 3].Age.to_numpy())) . AVG age for severity 0: 36.833333333333336 AVG age for severity 1: 47.45283018867924 AVG age for severity 2: 54.3125 AVG age for severity 3: 69.4 . 1. Data Wash . Remove NULL Features . remove_columns = [&#39;MedNum&#39;, &#39;LVEF&#39;, &#39;SO2&#39;, &#39;PO2&#39;, &#39;YHZS&#39;, &#39;RML&#39;, &#39;RUL&#39;, &#39;RLL&#39;, &#39;LUL&#39;, &#39;LLL&#39;] . covid = covid.drop(remove_columns, axis=1) . Remove Partially NULL Features . remove_columns = [&#39;Onset2Admi&#39;, &#39;Onset2CT1&#39;, &#39;Onset2CTPositive1&#39;, &#39;Onset2CTPeak&#39;] . covid = covid.drop(remove_columns, axis=1) . Remove &quot; &quot; Features . covid = covid[covid.Weight != &quot; &quot;] . covid = covid[covid.cTnI != &quot; &quot;] . String to Float . covid[&#39;Weight&#39;] = covid[&#39;Weight&#39;].astype(np.float64) covid[&#39;Height&#39;] = covid[&#39;Height&#39;].astype(np.float64) covid[&#39;cTnITimes&#39;] = covid[&#39;cTnITimes&#39;].astype(np.float64) covid[&#39;cTnI&#39;] = covid[&#39;cTnI&#39;].astype(np.float64) covid[&#39;NTproBNP&#39;] = covid[&#39;NTproBNP&#39;].astype(np.float64) covid[&#39;Cr&#39;] = covid[&#39;Cr&#39;].astype(np.float64) . 2. Train Test Split . Add New Features . covid[&#39;NSympton&#39;] = covid[&#39;Fever&#39;] + covid[&#39;Cough&#39;] + covid[&#39;Phlegm&#39;] + covid[&#39;Hemoptysis&#39;] + covid[&#39;SoreThroat&#39;] + covid[&#39;Catarrh&#39;] + covid[&#39;Headache&#39;] + covid[&#39;ChestPain&#39;] + covid[&#39;Fatigue&#39;] + covid[&#39;SoreMuscle&#39;]+covid[&#39;Stomachache&#39;] + covid[&#39;Diarrhea&#39;] + covid[&#39;PoorAppetite&#39;] + covid[&#39;NauseaNVomit&#39;] . covid[&#39;NDisease&#39;] = covid[&#39;Hypertention&#39;] + covid[&#39;Hyperlipedia&#39;] + covid[&#39;DM&#39;] + covid[&#39;Lung&#39;] + covid[&#39;CAD&#39;] + covid[&#39;Arrythmia&#39;] + covid[&#39;Cancer&#39;] . from sklearn import preprocessing from sklearn.model_selection import train_test_split . y = covid.Severity01.to_numpy() . # Use Both # covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;], axis=1) . # Use AIVolume # covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;, &quot;CTScore&quot;], axis=1) . # Use CTScore # covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;, &quot;AIVolumeP&quot;], axis=1) . # Use None covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;, &quot;CTScore&quot;, &quot;AIVolumeP&quot;], axis=1) . covid = covid.drop([&quot;No&quot;], axis=1) . X = covid X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.9, random_state = 1) . X_train.shape, X_test.shape . ((77, 57), (9, 57)) . X_train.columns . Index([&#39;Sex&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;Height&#39;, &#39;Weight&#39;, &#39;BMI&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;AST&#39;, &#39;LDH&#39;, &#39;CK&#39;, &#39;CKMB&#39;, &#39;HBDH&#39;, &#39;HiCKMB&#39;, &#39;NTproBNP&#39;, &#39;Cr&#39;, &#39;PCT1&#39;, &#39;WBC1&#39;, &#39;NEU1&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;PCT2&#39;, &#39;WBC2&#39;, &#39;NEU2&#39;, &#39;LYM2&#39;, &#39;N2L2&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Sympton&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;Hemoptysis&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, &#39;Stomachache&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, &#39;CAD&#39;, &#39;Arrythmia&#39;, &#39;Cancer&#39;, &#39;NSympton&#39;, &#39;NDisease&#39;], dtype=&#39;object&#39;) . y_test . array([0, 1, 0, 0, 1, 1, 0, 1, 0], dtype=int64) . 3. Feature Selection . 3.1 Basic Methods . 3.1.1 Drop constant and Quasi-constant features . Maybe those outliers are severe patients . from sklearn.feature_selection import VarianceThreshold . def drop_features(X_train, X_test, threshhold): sel = VarianceThreshold(threshold=threshhold) sel.fit(X_train) print(&quot;No. of constant features:&quot;, len([ x for x in X_train.columns if x not in X_train.columns[sel.get_support()] ]) ) constant_features = [x for x in X_train.columns if x not in X_train.columns[sel.get_support()]] print(constant_features) X_train.drop(labels=constant_features, axis=1, inplace=True) X_test.drop(labels=constant_features, axis=1, inplace=True) . Drop constant and quasi-constant features . drop_features(X_train, X_test, 0.01) . X_train.shape, X_test.shape . ((77, 55), (9, 55)) . 3.1.2 Drop Duplicated Features . Maybe some symptoms are correlated . covid_t = covid.T print(&quot;No. of Duplicated Features:&quot;, covid_t.duplicated().sum()) print(covid_t[covid_t.duplicated()].index.values) . No. of Duplicated Features: 1 [&#39;Arrythmia&#39;] . Print out duplicated features . duplicated_feat = [] for i in range(0, len(X_train.columns)): col_1 = X_train.columns[i] for col_2 in X_train.columns[i + 1 : ]: if X_train[col_1].equals(X_train[col_2]): print(col_1) print(col_2) duplicated_feat.append(col_2) . CAD Arrythmia . Drop duplicated features . # covid_unique = covid_t.drop_duplicates(keep=&#39;first&#39;).T . X_train.drop(labels=covid_t[covid_t.duplicated()].index.values, axis=1, inplace=True) X_test.drop(labels=covid_t[covid_t.duplicated()].index.values, axis=1, inplace=True) . X_train.shape, X_test.shape . ((77, 54), (9, 54)) . 3.2 Correlations . categorical_features = [&#39;Sex&#39;, &#39;AgeG1&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;Hemoptysis&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, # &#39;Stomachache&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, #&#39;CAD&#39;, &#39;Arrythmia&#39;, &#39;Cancer&#39;] . numerical_features = [&#39;Age&#39;, &#39;Height&#39;, &#39;Weight&#39;, &#39;BMI&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;AST&#39;, &#39;LDH&#39;, &#39;CK&#39;, &#39;CKMB&#39;, &#39;HBDH&#39;, &#39;HiCKMB&#39;, &#39;NTproBNP&#39;, &#39;Cr&#39;, &#39;PCT1&#39;, &#39;WBC1&#39;, &#39;NEU1&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;WBC2&#39;, &#39;NEU2&#39;, &#39;LYM2&#39;, &#39;N2L2&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;] . # numerics = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;] # numerical_vars = list(covid.select_dtypes(include=numerics).columns) # data = covid[numerical_vars] . corrmat = X_train.corr() . fig, ax = plt.subplots() fig.set_size_inches(11, 11) sns.heatmap(corrmat) . &lt;AxesSubplot:&gt; . corrmat = X_train.corr() corrmat = corrmat.abs().unstack() corrmat = corrmat.sort_values(ascending=False) corrmat = corrmat[corrmat &gt;= 0.8] corrmat = corrmat[corrmat &lt; 1] corrmat = pd.DataFrame(corrmat).reset_index() corrmat.columns = [&#39;feature1&#39;, &#39;feature2&#39;, &#39;corr&#39;] corrmat . feature1 feature2 corr . 0 Age | AgeG1 | 0.893413 | . 1 AgeG1 | Age | 0.893413 | . # find groups of correlated features grouped_feature_ls = [] correlated_groups = [] for feature in corrmat.feature1.unique(): if feature not in grouped_feature_ls: # find all features correlated to a single feature correlated_block = corrmat[corrmat.feature1 == feature] grouped_feature_ls = grouped_feature_ls + list( correlated_block.feature2.unique()) + [feature] # append the block of features to the list correlated_groups.append(correlated_block) print(&#39;found {} correlated groups&#39;.format(len(correlated_groups))) print(&#39;out of {} total features&#39;.format(X_train.shape[1])) . found 9 correlated groups out of 54 total features . # now we can visualise each group. We see that some groups contain # only 2 correlated features, some other groups present several features # that are correlated among themselves. for group in correlated_groups: print(group) print() . feature1 feature2 corr 0 LDH HBDH 0.958191 feature1 feature2 corr 2 Height PoorAppetite 0.911704 feature1 feature2 corr 4 WBC2 NEU2 0.911419 feature1 feature2 corr 6 WBC1 NEU1 0.90352 feature1 feature2 corr 8 Age AgeG1 0.893413 feature1 feature2 corr 10 cTnICKMBOrdinal2 cTnICKMBOrdinal1 0.853741 feature1 feature2 corr 12 LYM1 LYM2 0.842688 feature1 feature2 corr 14 BMI Weight 0.842409 feature1 feature2 corr 16 NTproBNP N2L2 0.808767 . def correlation(dataset, threshold): col_corr = set() corr_matrix = dataset.corr() for i in range(len(corr_matrix.columns)): for j in range(i): if abs(corr_matrix.iloc[i, j] &gt;= threshold): colname = corr_matrix.columns[i] col_corr.add(colname) return col_corr . corr_features = list((correlation(X_train, 0.8))) print(corr_features) . [&#39;HBDH&#39;, &#39;NEU1&#39;, &#39;AgeG1&#39;, &#39;NEU2&#39;, &#39;BMI&#39;, &#39;N2L2&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;LYM2&#39;] . for i in corr_features: if i in categorical_features: corr_features.remove(i) . for i in corr_features: if i in numerical_features: numerical_features.remove(i) for i in corr_features: if i in categorical_features: categorical_features.remove(i) . corr_features . [&#39;HBDH&#39;, &#39;NEU1&#39;, &#39;NEU2&#39;, &#39;BMI&#39;, &#39;N2L2&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;LYM2&#39;] . X_train.drop(labels=corr_features, axis=1, inplace=True) X_test.drop(labels=corr_features, axis=1, inplace=True) . d: anaconda3 envs covid19 lib site-packages pandas core frame.py:4167: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 47), (9, 47)) . 3.3 Statistical Methods . 3.3.1 Mutual Information . from sklearn.feature_selection import mutual_info_classif, mutual_info_regression from sklearn.feature_selection import SelectKBest, SelectPercentile . mi = mutual_info_classif(X_train, y_train) mi = pd.Series(mi) mi.index = X_train.columns . Features on the left side have more mutual information with y . mi.sort_values(ascending=False).plot.bar(figsize=(20, 8)) . &lt;AxesSubplot:&gt; . sel_ = SelectKBest(mutual_info_classif, k = 40).fit(X_train, y_train) . mi_features = list(X_train.columns[ ~ sel_.get_support()].values) . for i in mi_features: if i in categorical_features: mi_features.remove(i) . for i in mi_features: if i in numerical_features: numerical_features.remove(i) for i in mi_features: if i in categorical_features: categorical_features.remove(i) . mi_features . [&#39;Height&#39;, &#39;Temp&#39;, &#39;CK&#39;, &#39;HiCKMB&#39;, &#39;Cr&#39;, &#39;PCT1&#39;] . X_train.drop(labels=mi_features, axis=1, inplace=True) X_test.drop(labels=mi_features, axis=1, inplace=True) . d: anaconda3 envs covid19 lib site-packages pandas core frame.py:4167: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 41), (9, 41)) . 3.3.2 Fisher Score . from sklearn.feature_selection import chi2 . categorical features . f_score = chi2(X_train[categorical_features], y_train) . The smaller ones have more correlations . p_values = pd.Series(f_score[1]) p_values.index = X_train[categorical_features].columns p_values.sort_values(ascending=False) . Cancer 0.887949 SoreThroat 0.842057 Cough 0.703238 Headache 0.638344 Hemoptysis 0.594525 ChestPain 0.356552 NauseaNVomit 0.356552 Diarrhea 0.333947 Sex 0.302537 Fever 0.202574 Catarrh 0.159040 Hypertention 0.154388 SoreMuscle 0.105717 Hyperlipedia 0.099153 Lung 0.062605 PoorAppetite 0.060289 Phlegm 0.046410 AgeG1 0.037459 DM 0.008457 Fatigue 0.000049 dtype: float64 . p_values[p_values&lt;0.05].index.values . array([&#39;AgeG1&#39;, &#39;Phlegm&#39;, &#39;Fatigue&#39;, &#39;DM&#39;], dtype=object) . # for c in categorical_features: # if c not in p_values[p_values&lt;0.05].index.values: # categorical_features.remove(c) # print(c) # X_train.drop(labels=c, axis=1, inplace=True) # X_test.drop(labels=c, axis=1, inplace=True) . X_train.shape, X_test.shape . ((77, 41), (9, 41)) . 3.3.3 Univariate . Non-categorical features . from sklearn.feature_selection import f_classif, f_regression . univariate = f_classif(X_train[numerical_features], y_train) univariate = pd.Series(univariate[1]) univariate.index = X_train[numerical_features].columns univariate.sort_values(ascending=False, inplace=True) . univariate.sort_values(ascending=False).plot.bar(figsize=(20, 8)) . &lt;AxesSubplot:&gt; . univariate[univariate &gt; 0.05] . WBC1 0.430778 AST 0.376614 CKMB 0.351257 WBC2 0.207957 Weight 0.191531 dtype: float64 . for n in numerical_features: if n in univariate[univariate &gt; 0.05].index.values: numerical_features.remove(n) print(n) X_train.drop(labels=n, axis=1, inplace=True) X_test.drop(labels=n, axis=1, inplace=True) . X_train.shape . (77, 36) . 3.3.4 ROC-AUC . from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.metrics import roc_auc_score, mean_squared_error . # loop to build a tree, make predictions and get the roc-auc # for each feature of the train set roc_values = [] for feature in X_train.columns: clf = DecisionTreeClassifier() clf.fit(X_train[feature].fillna(0).to_frame(), y_train) y_scored = clf.predict_proba(X_test[feature].fillna(0).to_frame()) roc_values.append(roc_auc_score(y_test, y_scored[:, 1])) . # let&#39;s add the variable names and order it for clearer visualisation roc_values = pd.Series(roc_values) roc_values.index = X_train.columns roc_values.sort_values(ascending=False) . cTnITimes 0.775 CRP2 0.675 Fever 0.675 NTproBNP 0.650 SoreThroat 0.650 Headache 0.625 LYM1 0.625 ALB2 0.625 N2L1 0.625 NSympton 0.600 AgeG1 0.550 Phlegm 0.500 Hemoptysis 0.500 cTnICKMBOrdinal1 0.500 ChestPain 0.500 Fatigue 0.500 SoreMuscle 0.500 Diarrhea 0.500 PoorAppetite 0.500 NauseaNVomit 0.500 cTnI 0.500 Lung 0.500 CAD 0.500 Age 0.500 CRP1 0.500 LDH 0.500 Sex 0.450 ALB1 0.450 Hypertention 0.425 Hyperlipedia 0.400 DM 0.400 Cancer 0.400 Sympton 0.375 Catarrh 0.375 NDisease 0.375 Cough 0.325 dtype: float64 . # and now let&#39;s plot roc_values.sort_values(ascending=False).plot.bar(figsize=(20, 8)) . &lt;AxesSubplot:&gt; . # a roc auc value of 0.5 indicates random decision # let&#39;s check how many features show a roc-auc value # higher than random len(roc_values[roc_values &gt; 0.5]) . 11 . roc_values[roc_values &lt; 0.5] . Sex 0.450 LDH 0.500 ALB1 0.450 Sympton 0.375 Cough 0.325 Catarrh 0.375 Hypertention 0.425 Hyperlipedia 0.400 DM 0.400 Cancer 0.400 NDisease 0.375 dtype: float64 . roc_features = roc_values[roc_values &lt; 0.5].index.values . for i in roc_features: if i in numerical_features: numerical_features.remove(i) for i in roc_features: if i in categorical_features: categorical_features.remove(i) . roc_features . array([&#39;Sex&#39;, &#39;LDH&#39;, &#39;ALB1&#39;, &#39;Sympton&#39;, &#39;Cough&#39;, &#39;Catarrh&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Cancer&#39;, &#39;NDisease&#39;], dtype=object) . # X_train.drop(labels=roc_features, axis=1, inplace=True) # X_test.drop(labels=roc_features, axis=1, inplace=True) . X_train.shape, X_test.shape . ((77, 36), (9, 36)) . 4. Feature Engineering . Feature Tools . #collapse-hide # import featuretools as ft . . 5. Classifier . import sklearn import sklearn.ensemble import sklearn.metrics import xgboost as xgb . Cross Validation . from sklearn.model_selection import cross_val_score . def cv_score(classifier, X, y, scoring): return cross_val_score(classifier, X, y, cv=5, scoring=scoring) . Decision Tree . dt = sklearn.tree.DecisionTreeClassifier() dt_f1 = cv_score(dt, X_train, y_train, &#39;f1&#39;) dt.fit(X_train, y_train) . DecisionTreeClassifier() . from sklearn import tree . tree.plot_tree(dt) . [Text(148.8, 201.90857142857143, &#39;X[7] &lt;= 1140.0 ngini = 0.344 nsamples = 77 nvalue = [60, 17]&#39;), Text(111.60000000000001, 170.84571428571428, &#39;X[12] &lt;= 14.615 ngini = 0.187 nsamples = 67 nvalue = [60, 7]&#39;), Text(74.4, 139.78285714285715, &#39;gini = 0.0 nsamples = 45 nvalue = [45, 0]&#39;), Text(148.8, 139.78285714285715, &#39;X[7] &lt;= 190.0 ngini = 0.434 nsamples = 22 nvalue = [15, 7]&#39;), Text(74.4, 108.72, &#39;X[4] &lt;= 0.016 ngini = 0.18 nsamples = 10 nvalue = [9, 1]&#39;), Text(37.2, 77.65714285714284, &#39;gini = 0.0 nsamples = 9 nvalue = [9, 0]&#39;), Text(111.60000000000001, 77.65714285714284, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 1]&#39;), Text(223.20000000000002, 108.72, &#39;X[13] &lt;= 35.25 ngini = 0.5 nsamples = 12 nvalue = [6, 6]&#39;), Text(186.0, 77.65714285714284, &#39;gini = 0.0 nsamples = 3 nvalue = [3, 0]&#39;), Text(260.40000000000003, 77.65714285714284, &#39;X[8] &lt;= 1.48 ngini = 0.444 nsamples = 9 nvalue = [3, 6]&#39;), Text(223.20000000000002, 46.59428571428572, &#39;X[11] &lt;= 39.7 ngini = 0.48 nsamples = 5 nvalue = [3, 2]&#39;), Text(186.0, 15.531428571428563, &#39;gini = 0.0 nsamples = 3 nvalue = [3, 0]&#39;), Text(260.40000000000003, 15.531428571428563, &#39;gini = 0.0 nsamples = 2 nvalue = [0, 2]&#39;), Text(297.6, 46.59428571428572, &#39;gini = 0.0 nsamples = 4 nvalue = [0, 4]&#39;), Text(186.0, 170.84571428571428, &#39;gini = 0.0 nsamples = 10 nvalue = [0, 10]&#39;)] . tree.export_graphviz(dt, out_file=&quot;tree.dot&quot;, feature_names = X_train.columns, class_names=[&#39;Normal&#39;, &#39;Severe&#39;], filled = True) . Random Forest . from sklearn.ensemble import RandomForestClassifier . rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100) rf_f1 = cv_score(rf, X_train, y_train, &#39;f1&#39;) rf.fit(X_train, y_train) . RandomForestClassifier() . # we get the feature importance attributed by the # random forest model (more on this in coming lectures) importance = pd.concat( [pd.Series(X_train.columns), pd.Series(rf.feature_importances_)], axis=1) importance.columns = [&#39;feature&#39;, &#39;importance&#39;] importance.sort_values(by=&#39;importance&#39;, ascending=False) . feature importance . 7 NTproBNP | 0.177084 | . 12 CRP2 | 0.131552 | . 3 cTnITimes | 0.094925 | . 10 CRP1 | 0.087578 | . 13 ALB2 | 0.071429 | . 4 cTnI | 0.067491 | . 6 LDH | 0.065550 | . 9 N2L1 | 0.043029 | . 11 ALB1 | 0.041507 | . 8 LYM1 | 0.041117 | . 1 Age | 0.032757 | . 23 Fatigue | 0.027832 | . 35 NDisease | 0.016272 | . 17 Phlegm | 0.015435 | . 34 NSympton | 0.013927 | . 5 cTnICKMBOrdinal1 | 0.011789 | . 0 Sex | 0.007919 | . 30 DM | 0.006794 | . 31 Lung | 0.006588 | . 28 Hypertention | 0.005127 | . 15 Fever | 0.004636 | . 2 AgeG1 | 0.004457 | . 19 SoreThroat | 0.004318 | . 29 Hyperlipedia | 0.003800 | . 32 CAD | 0.003774 | . 33 Cancer | 0.002551 | . 24 SoreMuscle | 0.002306 | . 16 Cough | 0.002206 | . 27 NauseaNVomit | 0.002140 | . 25 Diarrhea | 0.001959 | . 21 Headache | 0.000873 | . 22 ChestPain | 0.000614 | . 26 PoorAppetite | 0.000431 | . 14 Sympton | 0.000231 | . 20 Catarrh | 0.000000 | . 18 Hemoptysis | 0.000000 | . XGBoost . # Create a model # Params from: https://www.kaggle.com/aharless/swetha-s-xgboost-revised xgbc = xgb.XGBClassifier( max_depth = 4, subsample = 0.8, colsample_bytree = 0.7, colsample_bylevel = 0.7, scale_pos_weight = 9, min_child_weight = 0, reg_alpha = 4, objective = &#39;binary:logistic&#39; ) xgbc_f1 = cv_score(xgbc, X_train, y_train, &#39;f1&#39;) # Fit the models xgbc.fit(X_train, y_train) . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=0.7, colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=4, min_child_weight=0, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=4, reg_lambda=1, scale_pos_weight=9, subsample=0.8, tree_method=&#39;exact&#39;, validate_parameters=1, verbosity=None) . Neural Networks . import keras from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout . from keras.wrappers.scikit_learn import KerasClassifier from sklearn.model_selection import cross_val_score . def build_classifier() : nn = Sequential() nn.add(Dense(activation=&#39;relu&#39;, input_dim=X_train.shape[1], units=10)) nn.add(Dropout(rate = 0.1)) nn.add(Dense(kernel_initializer=&quot;uniform&quot;, activation=&#39;relu&#39;, units=15)) nn.add(Dropout(rate = 0.1)) nn.add(Dense(kernel_initializer=&quot;uniform&quot;, activation=&#39;relu&#39;, units=5)) nn.add(Dropout(rate = 0.1)) nn.add(Dense(kernel_initializer=&#39;uniform&#39;,activation=&#39;sigmoid&#39;, units=1)) nn.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return nn . nn = KerasClassifier(build_fn=build_classifier, epochs=800, batch_size=50, verbose=0) # nn = build_classifier(); . history = nn.fit(X_train, y_train, batch_size=50, epochs=800, validation_split = 0.2) #verbose = 2 . print(history.history.keys()) . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]) . plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Model Loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;],loc=&#39;upper left&#39;) plt.show() . nn_f1 = cv_score(nn, X_train, y_train, &#39;f1&#39;) . WARNING:tensorflow:6 out of the last 6 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001ED936BF400&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. WARNING:tensorflow:7 out of the last 7 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001ED935CFEA0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. WARNING:tensorflow:8 out of the last 8 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001ED96D942F0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. WARNING:tensorflow:9 out of the last 9 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001ED94A3C400&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. WARNING:tensorflow:10 out of the last 10 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001ED98264268&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. . 6. Results . Cross Validation . print(&quot;Decision Tree&quot;) # print(&#39;Precision&#39;, np.mean(dt_precision)) # print(&#39;Recall&#39;, np.mean(dt_recall)) print(&#39;F1&#39;, np.mean(dt_f1)) print() print(&quot;Random Forest&quot;) # print(&#39;Precision&#39;, np.mean(rf_precision)) # print(&#39;Recall&#39;, np.mean(rf_recall)) print(&#39;F1&#39;, np.mean(rf_f1)) print() print(&quot;XGBoost&quot;) # print(&#39;Precision&#39;, np.mean(xgbc_precision)) # print(&#39;Recall&#39;, np.mean(xgbc_recall)) print(&#39;F1&#39;, np.mean(xgbc_f1)) print() print(&quot;NN&quot;) # print(&#39;Precision&#39;, np.mean(xgbc_precision)) # print(&#39;Recall&#39;, np.mean(xgbc_recall)) print(&#39;F1&#39;, np.mean(nn_f1)) . Save Models . import pickle . with open(&quot;model-3.pkl&quot;, &#39;wb&#39;) as f: pickle.dump([dt, rf, xgbc], f) # with open(&quot;model-4.pkl&quot;, &#39;wb&#39;) as f: # pickle.dump([dt, rf, xgbc, nn], f) with open(&quot;dataset.pkl&quot;, &#39;wb&#39;) as f: pickle.dump([X_train, X_test, y_train, y_test], f) . nn.model.save(&#39;nn.h5&#39;) . Load Keras Model . from keras.models import load_model . nn = load_model(&quot;nn.h5&quot;) . Load Models . with open(&quot;model-4.pkl&quot;, &#39;rb&#39;) as f: [dt, rf, xgbc, nn] = pickle.load(f) . Load Dataset . with open(&quot;dataset.pkl&quot;, &#39;rb&#39;) as f: [X_train, X_test, y_train, y_test] = pickle.load(f) . X_train.shape . (77, 36) . X_test.shape . (9, 36) . Prediction . import sklearn . dt_pred = dt.predict(X_test) rf_pred = rf.predict(X_test) xgbc_pred = xgbc.predict(X_test) nn_pred = nn.predict(X_test) nn_pred[nn_pred &gt; 0.5] = 1 nn_pred[nn_pred &lt;= 0.5] = 0 . print(&quot;Decision Tree&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, dt_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, dt_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, dt_pred)) print() print(&quot;Random Forest&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, rf_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, rf_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, rf_pred)) print() print(&quot;XGBoost&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, xgbc_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, xgbc_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, xgbc_pred)) print() print(&quot;NN&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, nn_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, nn_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, nn_pred)) . Decision Tree Precision: 0.6666666666666666 Recal: 0.5 F1: 0.5714285714285715 Random Forest Precision: 0.5555555555555556 Recal: 0.25 F1: 0.3333333333333333 XGBoost Precision: 0.7777777777777778 Recal: 1.0 F1: 0.8 NN Precision: 0.7777777777777778 Recal: 1.0 F1: 0.8 . 7. Explain . 7.1 Partial Dependence Plot . from sklearn.inspection import plot_partial_dependence . X_train.columns . Index([&#39;Sex&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;LDH&#39;, &#39;NTproBNP&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Sympton&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, &#39;CAD&#39;, &#39;Cancer&#39;, &#39;NSympton&#39;, &#39;NDisease&#39;], dtype=&#39;object&#39;) . plot_partial_dependence(rf, X_train, [7]) . &lt;sklearn.inspection._partial_dependence.PartialDependenceDisplay at 0x25c840d8e80&gt; . nn._estimator_type = &#39;classifier&#39; . plot_partial_dependence(nn, X_train, [7, 12]) fig = plt.gcf() fig.subplots_adjust(hspace=0.3) fig.savefig(&quot;nn_pdp_crp_ntproBNP&quot;) . plot_partial_dependence(rf, X_train, [3, 4]) . &lt;sklearn.inspection._partial_dependence.PartialDependenceDisplay at 0x25c88523080&gt; . plot_partial_dependence(dt, X_train, [11, 13]) . &lt;sklearn.inspection._partial_dependence.PartialDependenceDisplay at 0x25c887849b0&gt; . plot_partial_dependence(rf, X_train, [1, 6, 8]) . &lt;sklearn.inspection._partial_dependence.PartialDependenceDisplay at 0x25c888a2898&gt; . plot_partial_dependence(rf, X_train, [17]) . &lt;sklearn.inspection._partial_dependence.PartialDependenceDisplay at 0x25c889fdd30&gt; . def my_pdp(model, X, feat_idx): X = np.array(X) fmax, fmin = np.max(X[:, feat_idx]), np.min(X[:, feat_idx]) frange = np.linspace(fmin, fmax, 100) preds = [] for x in frange: X_ = X.copy() X_[:, feat_idx] = x pred = model.predict(X_) preds.append(np.mean(pred)) return (frange, np.array(preds)) . my_data = my_pdp(dt, X_train, 6) # sk_data = partial_dependence(dtr, X = X_train, features = [6], percentiles=[0,1]) plt.subplot(121) plt.plot(my_data[0], my_data[1]) # plt.subplot(122) # plt.plot(sk_data[1][0], sk_data[0][0]) plt.show() . 7.2 Individual Conditional Expectation (ICE) . def my_ice(model, X, feat_idx): # X = np.array(X) fmax, fmin = np.max(np.array(X)[:, feat_idx]), np.min(np.array(X)[:, feat_idx]) frange = np.linspace(fmin, fmax, 100) preds = [] for x in frange: X_ = X.copy() X_.iloc[:, feat_idx] = x pred = model.predict_proba(X_) # print(pred.shape) preds.append(pred[:, 1]) return (frange, np.array(preds)) . X_train.columns . Index([&#39;Sex&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;LDH&#39;, &#39;NTproBNP&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Sympton&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, &#39;CAD&#39;, &#39;Cancer&#39;, &#39;NSympton&#39;, &#39;NDisease&#39;], dtype=&#39;object&#39;) . BNP_data = my_ice(nn, X_train, 8) CRP_data = my_ice(nn, X_train, 13) # sk_data = partial_dependence(dtr, X = X_train, features = [6], percentiles=[0,1]) . # parameters = {&#39;axes.labelsize&#39;: 15, # &#39;axes.titlesize&#39;: 20} # plt.rcParams.update(parameters) f = plt.figure(figsize=(10, 5)) plt.subplot(121) plt.plot(BNP_data[0], BNP_data[1]) plt.xlabel(&#39;NTproBNP&#39;) plt.xticks(fontsize=14) plt.yticks(fontsize=14) plt.ylabel(&#39;Individual Conditional Expectation&#39;) plt.subplot(122) plt.plot(CRP_data[0], CRP_data[1]) plt.xlabel(&#39;CRP2&#39;) plt.xticks(fontsize=14) plt.yticks(fontsize=14) plt.show() f.savefig(&#39;nn_ice_crp_ntproBNP.png&#39;) . 7.3 Accumulated Local Effects (ALE) Plot . from alibi.explainers import ALE, plot_ale . def predict_fn_xbg(X): X_data = pd.DataFrame(data=X, columns=xgbc.get_booster().feature_names) return xgbc.predict_proba(X_data) . def ale_plot(model, feature_names, target_names, X_train, features): my_ale = ALE(model.predict_proba, feature_names=feature_names, target_names=target_names) exp = my_ale.explain(X_train.to_numpy()) plot_ale(exp, features=features, fig_kw={&#39;figwidth&#39;:10, &#39;figheight&#39;: 5}) . def ale_plot_xgbc(model, feature_names, target_names, X_train, features): my_ale = ALE(predict_fn_xbg, feature_names=feature_names, target_names=target_names) exp = my_ale.explain(X_train.to_numpy()) plot_ale(exp, features=features, fig_kw={&#39;figwidth&#39;:10, &#39;figheight&#39;: 5}) . X_train.columns . Index([&#39;Sex&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;LDH&#39;, &#39;NTproBNP&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Sympton&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, &#39;CAD&#39;, &#39;Cancer&#39;, &#39;NSympton&#39;, &#39;NDisease&#39;], dtype=&#39;object&#39;) . parameters = {&#39;axes.labelsize&#39;: 15, &#39;axes.titlesize&#39;: 20} plt.rcParams.update(parameters) plt.rcParams.update({&#39;font.size&#39;: 14}) ale_plot(nn, X_train.columns, [&#39;normal&#39;, &#39;severe&#39;], X_train, [8, 13]) . parameters = {&#39;axes.labelsize&#39;: 15, &#39;axes.titlesize&#39;: 20} plt.rcParams.update(parameters) plt.rcParams.update({&#39;font.size&#39;: 14}) ale_plot_xgbc(xgbc, X_train.columns, [&#39;normal&#39;, &#39;severe&#39;], X_train, [8, 13]) . plt.figure() . 7.4 Feature Interaction . H-Statistic . 7.5 Permutation Feature Importance . from sklearn.inspection import permutation_importance from matplotlib import pyplot . results = permutation_importance(rf, X_train, y_train, scoring=&#39;accuracy&#39;) . # get importance importance = results.importances_mean # summarize feature importance for i,v in enumerate(importance): if v != 0: print(&#39;Feature: %20s, t Score: %.3f t Actual: %.3f&#39; % (X_train.columns[i],v, rf.feature_importances_[i])) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() . Feature: cTnI, Score: 0.013 Actual: 0.069 Feature: NTproBNP, Score: 0.018 Actual: 0.160 Feature: LYM1, Score: 0.008 Actual: 0.042 Feature: CRP1, Score: 0.010 Actual: 0.085 Feature: ALB1, Score: 0.003 Actual: 0.046 Feature: CRP2, Score: 0.029 Actual: 0.125 Feature: ALB2, Score: 0.008 Actual: 0.052 Feature: Hypertention, Score: 0.013 Actual: 0.008 Feature: NDisease, Score: 0.010 Actual: 0.019 . pyplot.bar([x for x in range(len(rf.feature_importances_))], rf.feature_importances_) pyplot.show() . 7.6 Global Surrogate . R-squared measure . y_s = rf.predict_proba(X_test) y_o = nn.predict_proba(X_test) . 1 - ( np.sum(np.power(y_s - y_o, 2)) / np.sum(np.power(y_o - np.mean(y_o), 2)) ) . 0.758393421249955 . 7.7 LIME . from lime import lime_tabular . X_train.columns . Index([&#39;Sex&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;LDH&#39;, &#39;NTproBNP&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Sympton&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, &#39;CAD&#39;, &#39;Cancer&#39;, &#39;NSympton&#39;, &#39;NDisease&#39;], dtype=&#39;object&#39;) . categorical_features = [0, 2, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] categorical_names = {} for c in categorical_features: categorical_names[c] = [&quot;False&quot;, &quot;True&quot;] . idx = 7 print(&#39;Probability(normal) =&#39;, rf.predict_proba(np.array(X_test)[idx, :].reshape(1, -1))[0][0]) . Probability(normal) = 0.52 . print(&quot;Pred: &quot;, rf_pred.reshape(1, -1)[0]) print(&quot;True: &quot;, y_test) . Pred: [0 0 1 0 1 0 0 0 0] True: [0 1 0 0 1 1 0 1 0] . y_test.shape . (9,) . [1, 5, 7] . [0, 5, 7] idx = 5 class_names = [&#39;normal&#39;, &#39;severe&#39;] print(&#39;Patient id: %d&#39; % idx) print(&#39;Probability(normal) =&#39;, nn.predict_proba(np.array(X_test)[idx, :].reshape(1, -1))[0][0]) print(&#39;True class: %s&#39; % class_names[y_test[idx]]) explainer = lime_tabular.LimeTabularExplainer(np.array(X_train), feature_names= X_train.columns, class_names = class_names, categorical_features = categorical_features, categorical_names = categorical_names, discretize_continuous=True) exp = explainer.explain_instance(np.array(X_test)[idx, :], predict_fn = nn.predict_proba, num_features = 10) %matplotlib inline fig = exp.as_pyplot_figure() . Patient id: 5 Probability(normal) = 0.3308184 True class: severe . # exp.save_to_file(&#39;./covid19.html&#39;) . exp.show_in_notebook(show_table=True, show_all=False) . . . # sc.inverse_transform(X_test[idx]) . exp.as_list() . xgbc.predict_proba(X_test[5:6]) . array([[0.26909047, 0.7309095 ]], dtype=float32) . def predict_fn_xbg(X): X_data = pd.DataFrame(data=X, columns=xgbc.get_booster().feature_names) return xgbc.predict_proba(X_data) . idx = 5 print(&#39;Patient id: %d&#39; % idx) print(&#39;Probability(normal) =&#39;, xgbc.predict_proba(X_test[idx:idx+1])[0][0]) print(&#39;True class: %s&#39; % class_names[y_test[idx]]) explainer = lime_tabular.LimeTabularExplainer(np.array(X_train), feature_names= X_train.columns, class_names = [&#39;normal&#39;, &#39;severe&#39;], categorical_features = categorical_features, categorical_names = categorical_names, discretize_continuous=True) exp = explainer.explain_instance(np.array(X_test)[idx, :], predict_fn = predict_fn_xbg, num_features = 10) %matplotlib inline fig = exp.as_pyplot_figure() . Patient id: 5 Probability(normal) = 0.26909047 True class: severe . More Train Cases . y_pred = xgbc.predict(X_train) y_predprob = xgbc.predict_proba(X_train) . for i, j in enumerate(y_predprob): if np.abs(j[0] - j[1]) &lt; 0.2 * 2: print(j) print(i, &#39;Close&#39;, y_train[i]) if y_pred[i] != y_train[i]: print(j) print(&quot;[&quot;, i, &quot;]&quot;, &quot;Predict:&quot;, y_pred[i], &quot;Truth:&quot;, y_train[i]) . exp = lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names, categorical_features=categorical_features, categorical_names = categorical_names, discretize_continuous=True) . idx = 27 exp = exp.explain_instance(X_train[idx, :], xgbc.predict_proba, num_features=10) exp.show_in_notebook() . for i, f in enumerate(feature_names): print(f, X_train[27][i]) . More test cases . y_predt = xgbc.predict(X_test) y_predprobt = xgbc.predict_proba(X_test) . for i, j in enumerate(y_predprobt): if np.abs(j[0] - j[1]) &lt; 0.2 * 2: print(j) print(i, &#39;Close&#39;, y_test[i]) if y_predt[i] != y_test[i]: print(j) print(&quot;[&quot;, i, &quot;]&quot;, &quot;Predict:&quot;, y_predt[i], &quot;Truth:&quot;, y_test[i]) . exp = lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names, categorical_features=categorical_features, categorical_names = categorical_names, discretize_continuous=True) . idx = 2 exp = exp.explain_instance(X_test[idx, :], xgbc.predict_proba, num_features=10) exp.show_in_notebook() . y_predprobt . y_test . for i, f in enumerate(feature_names): print(f, X_test[2][i]) . 7.8 Scoped Rules (Anchors) . from alibi.explainers import AnchorTabular from alibi.utils.data import gen_category_map . explainer = AnchorTabular(rf.predict, X_train.columns) . explainer.fit(X_train.to_numpy(), disc_perc=[25, 50, 75]) . idx = 1 print(&#39;Prediction: &#39;, explainer.predictor(X_test.to_numpy()[idx].reshape(1, -1))[0]) print(y_test[idx]) . explanation = explainer.explain(X_test.to_numpy()[idx].reshape(1, -1), threshold=0.8) print(&#39;Anchor: %s&#39; % (&#39; AND &#39;.join(explanation.anchor))) print(&#39;Precision: %.2f&#39; % explanation.precision) print(&#39;Coverage: %.2f&#39; % explanation.coverage) . 7.9 Shapley Values . Game Theory . 7.10 SHAP (SHapley Additive exPlanations) . import shap shap.initjs() . from alibi.explainers import KernelShap . Kernel Shap . def predict_fn_xbg(X): X_data = pd.DataFrame(data=X, columns=xgbc.get_booster().feature_names) return xgbc.predict(X_data) . explainer = KernelShap(rf.predict_proba) . explainer.fit(X_train) . KernelShap(meta={ &#39;name&#39;: &#39;KernelShap&#39;, &#39;type&#39;: [&#39;blackbox&#39;], &#39;task&#39;: &#39;classification&#39;, &#39;explanations&#39;: [&#39;local&#39;, &#39;global&#39;], &#39;params&#39;: { &#39;groups&#39;: None, &#39;group_names&#39;: None, &#39;weights&#39;: None, &#39;kwargs&#39;: {}, &#39;summarise_background&#39;: False } }) . explanation = explainer.explain(X_test.to_numpy()[5].reshape(1, -1)) . l1_reg=&#34;auto&#34; is deprecated and in the next version (v0.29) the behavior will change from a conditional use of AIC to simply &#34;num_features(10)&#34;! . . l1_reg=&#34;auto&#34; is deprecated and in the next version (v0.29) the behavior will change from a conditional use of AIC to simply &#34;num_features(10)&#34;! . shap.force_plot( explanation.expected_value[0], explanation.shap_values[0][0, :] , X_test.to_numpy()[5].reshape(1, -1), X_train.columns, ) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. explanation.shap_values[0] . array([[-0.00308013, 0.00255641, -0.00352397, 0.02371188, 0.00339383, 0.01556609, 0.00225176, -0.0223011 , -0.01894653, 0.00559539, -0.00483903, -0.05931478, -0.00148271, -0.02507473, 0.00206165, 0.00501227, 0.00101676, -0.00072845, -0.00392786, 0.00225693, 0. , 0. , 0. , 0.0043339 , 0.00216046, 0.00110417, 0. , 0. , -0.00400927, 0. , 0.00258468, 0. , 0. , 0. , 0.00291564, -0.00552702]]) . Metrics . from aix360.metrics import faithfulness_metric, monotonicity_metric . coefs = permutation_importance(rf, X_train, y_train, scoring=&#39;accuracy&#39;).importances_mean . x = X_test.iloc[0, :] base = np.zeros(x.shape[0]) . print(&quot;Faithfulness: &quot;, faithfulness_metric(rf, x.to_numpy(), coefs, base)) . Faithfulness: 0.3019123790292113 . print(&quot;Monotonity: &quot;, monotonicity_metric(rf, x.to_numpy(), coefs, base)) . Monotonity: False .",
            "url": "https://wuhanstudio.github.io/notebooks/jupyter/2020/11/12/covid.html",
            "relUrl": "/jupyter/2020/11/12/covid.html",
            "date": " â€¢ Nov 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Interpretable ML - Linear Regression",
            "content": "import pandas as pd . import numpy as np np.set_printoptions(suppress=True) . Read Dataset . bike_day = pd.read_csv(&quot;dataset/bike/day.csv&quot;) . bike_day.head() . instant dteday season yr mnth holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt . 0 1 | 2011-01-01 | 1 | 0 | 1 | 0 | 6 | 0 | 2 | 0.344167 | 0.363625 | 0.805833 | 0.160446 | 331 | 654 | 985 | . 1 2 | 2011-01-02 | 1 | 0 | 1 | 0 | 0 | 0 | 2 | 0.363478 | 0.353739 | 0.696087 | 0.248539 | 131 | 670 | 801 | . 2 3 | 2011-01-03 | 1 | 0 | 1 | 0 | 1 | 1 | 1 | 0.196364 | 0.189405 | 0.437273 | 0.248309 | 120 | 1229 | 1349 | . 3 4 | 2011-01-04 | 1 | 0 | 1 | 0 | 2 | 1 | 1 | 0.200000 | 0.212122 | 0.590435 | 0.160296 | 108 | 1454 | 1562 | . 4 5 | 2011-01-05 | 1 | 0 | 1 | 0 | 3 | 1 | 1 | 0.226957 | 0.229270 | 0.436957 | 0.186900 | 82 | 1518 | 1600 | . Feature Engineering . Numerical Features . bike_day.temp = bike_day.temp * (39 - (-8)) + (-8) bike_day.atemp = bike_day.atemp * (50 - (16)) + (16) bike_day.windspeed = bike_day.windspeed * 67 bike_day.hum = bike_day.hum * 100 . Datetime . bike_start_day = pd.to_datetime(&#39;2011-01-01&#39;) bike_day[&#39;dteday&#39;] = pd.to_datetime(bike_day[&#39;dteday&#39;]) bike_day[&#39;days_since_2011&#39;] = (bike_day.dteday - bike_start_day).dt.days . selected_features = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;, &quot;days_since_2011&quot;] X = bike_day[selected_features] y = bike_day.cnt . Categorical Features . Year . bike_day.yr[bike_day.loc[:, &#39;yr&#39;] == 0] = &#39;2011&#39; bike_day.yr[bike_day.loc[:, &#39;yr&#39;] == 1] = &#39;2012&#39; . &lt;ipython-input-222-fa42af29cf76&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.yr[bike_day.loc[:, &#39;yr&#39;] == 0] = &#39;2011&#39; . X = X.join(pd.get_dummies(bike_day.yr)) . Season . bike_day.season[bike_day.season == 1] = &#39;SPRING&#39; bike_day.season[bike_day.season == 2] = &#39;SUMMER&#39; bike_day.season[bike_day.season == 3] = &#39;FALL&#39; bike_day.season[bike_day.season == 4] = &#39;WINTER&#39; . &lt;ipython-input-224-6a2d6c4210e2&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.season[bike_day.season == 1] = &#39;SPRING&#39; . X = X.join(pd.get_dummies(bike_day.season)) . Weather . bike_day.weathersit[bike_day.weathersit == 1] = &#39;GOOD&#39; bike_day.weathersit[bike_day.weathersit == 2] = &#39;MISTY&#39; bike_day.weathersit[bike_day.weathersit == 3] = &#39;RAIN/SNOW/STORM&#39; . &lt;ipython-input-226-3f30ca62512e&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.weathersit[bike_day.weathersit == 1] = &#39;GOOD&#39; &lt;ipython-input-226-3f30ca62512e&gt;:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.weathersit[bike_day.weathersit == 2] = &#39;MISTY&#39; &lt;ipython-input-226-3f30ca62512e&gt;:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.weathersit[bike_day.weathersit == 3] = &#39;RAIN/SNOW/STORM&#39; . X = X.join(pd.get_dummies(bike_day.weathersit)) . Month . bike_day.mnth[bike_day.mnth == 1] = &#39;JAN&#39; bike_day.mnth[bike_day.mnth == 2] = &#39;FEB&#39; bike_day.mnth[bike_day.mnth == 3] = &#39;MAR&#39; bike_day.mnth[bike_day.mnth == 4] = &#39;APR&#39; bike_day.mnth[bike_day.mnth == 5] = &#39;MAY&#39; bike_day.mnth[bike_day.mnth == 6] = &#39;JUN&#39; bike_day.mnth[bike_day.mnth == 7] = &#39;JUL&#39; bike_day.mnth[bike_day.mnth == 8] = &#39;AUG&#39; bike_day.mnth[bike_day.mnth == 9] = &#39;SEP&#39; bike_day.mnth[bike_day.mnth == 10] = &#39;OCT&#39; bike_day.mnth[bike_day.mnth == 11] = &#39;NOV&#39; bike_day.mnth[bike_day.mnth == 12] = &#39;DEC&#39; . &lt;ipython-input-228-c8d46e89c532&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.mnth[bike_day.mnth == 1] = &#39;JAN&#39; . X = X.join(pd.get_dummies(bike_day.mnth)) . Train test split . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(), train_size = 0.7, random_state = 0) . Training . from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge . Ridge . rr = Ridge() . rr.fit(X_train, y_train) . Ridge() . Lasso . lr = Lasso(normalize=True) . lr.fit(X_train, y_train) . Lasso(normalize=True) . Metrics . from sklearn.metrics import mean_squared_error . Ridge . y_pred_r = rr.predict(X_test) . mean_squared_error(y_test, y_pred_r) . 650885.6073230483 . X.columns . Index([&#39;temp&#39;, &#39;atemp&#39;, &#39;hum&#39;, &#39;windspeed&#39;, &#39;days_since_2011&#39;, &#39;2011&#39;, &#39;2012&#39;, &#39;FALL&#39;, &#39;SPRING&#39;, &#39;SUMMER&#39;, &#39;WINTER&#39;, &#39;GOOD&#39;, &#39;MISTY&#39;, &#39;RAIN/SNOW/STORM&#39;, &#39;APR&#39;, &#39;AUG&#39;, &#39;DEC&#39;, &#39;FEB&#39;, &#39;JAN&#39;, &#39;JUL&#39;, &#39;JUN&#39;, &#39;MAR&#39;, &#39;MAY&#39;, &#39;NOV&#39;, &#39;OCT&#39;, &#39;SEP&#39;], dtype=&#39;object&#39;) . rr.coef_ . array([ 88.56269292, 26.35233461, -20.35055709, -50.15770336, 0.11737953, -931.02767102, 931.02767102, 129.9776705 , -858.75619916, 52.74840848, 676.03012019, 633.50374363, 356.83084875, -990.33459238, 77.61987747, -103.53517043, -229.15948447, -66.61631165, -164.93492354, -531.25800384, -128.86883146, 332.58997771, 377.06743755, -389.4554859 , 245.35738634, 581.19353222]) . rr.intercept_ . 3670.8640712920055 . Lasso . y_pred_l = lr.predict(X_test) . mean_squared_error(y_test, y_pred_l) . 659705.342130416 . lr.coef_ . array([ 75.4720349 , 33.18166704, -16.06025245, -43.72312824, 0. , -1891.66068971, 0. , -0. , -1045.36798995, 0. , 240.19341828, 283.91219444, -0. , -1414.16952065, 0. , 0. , -0. , -0. , -116.21917705, -301.81734998, -0. , 251.78401898, 273.0091002 , -13.95883721, 504.19655444, 651.96621325]) . lr.intercept_ . 4739.781565530179 . Plot . import altair as alt . lr_pred_df = pd.DataFrame({&#39;day&#39;: X_test[:, 4], &#39;y_test&#39;: y_test, &#39;y_pred&#39;: y_pred_l, }) . lr_pred_df = lr_pred_df.melt(&#39;day&#39;, var_name=&#39;data&#39;, value_name=&#39;value&#39;) . scales = alt.selection_interval(bind=&#39;scales&#39;) alt.Chart(lr_pred_df).mark_line(point=True).encode( x = &#39;day:Q&#39;, y = &#39;value:Q&#39;, color=&#39;data:N&#39;, tooltip=[ &#39;data:N&#39;, &#39;value:N&#39;] ).add_selection( scales ).properties(width=800) .",
            "url": "https://wuhanstudio.github.io/notebooks/jupyter/2020/05/27/iml-lr.html",
            "relUrl": "/jupyter/2020/05/27/iml-lr.html",
            "date": " â€¢ May 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://wuhanstudio.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, itâ€™s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://wuhanstudio.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://wuhanstudio.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}