{
  
    
        "post0": {
            "title": "Interpretable ML - COVID19",
            "content": "0. Load Data . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . np.set_printoptions(suppress=True) . covid = pd.read_csv(&quot;dataset/covid.csv&quot;) . print(&quot;AVG age for severity 0:&quot;, np.mean(covid[covid.Severity03 == 0].Age.to_numpy())) print(&quot;AVG age for severity 1:&quot;, np.mean(covid[covid.Severity03 == 1].Age.to_numpy())) print(&quot;AVG age for severity 2:&quot;, np.mean(covid[covid.Severity03 == 2].Age.to_numpy())) print(&quot;AVG age for severity 3:&quot;, np.mean(covid[covid.Severity03 == 3].Age.to_numpy())) . AVG age for severity 0: 36.833333333333336 AVG age for severity 1: 47.45283018867924 AVG age for severity 2: 54.3125 AVG age for severity 3: 69.4 . 1. Data Wash . Remove NULL Features . remove_columns = [&#39;MedNum&#39;, &#39;LVEF&#39;, &#39;SO2&#39;, &#39;PO2&#39;, &#39;YHZS&#39;, &#39;RML&#39;, &#39;RUL&#39;, &#39;RLL&#39;, &#39;LUL&#39;, &#39;LLL&#39;] . covid = covid.drop(remove_columns, axis=1) . Remove Partially NULL Features . remove_columns = [&#39;Onset2Admi&#39;, &#39;Onset2CT1&#39;, &#39;Onset2CTPositive1&#39;, &#39;Onset2CTPeak&#39;] . covid = covid.drop(remove_columns, axis=1) . Remove &quot; &quot; Features . covid = covid[covid.Weight != &quot; &quot;] . covid = covid[covid.cTnI != &quot; &quot;] . String to Float . covid[&#39;Weight&#39;] = covid[&#39;Weight&#39;].astype(np.float64) covid[&#39;Height&#39;] = covid[&#39;Height&#39;].astype(np.float64) covid[&#39;cTnITimes&#39;] = covid[&#39;cTnITimes&#39;].astype(np.float64) covid[&#39;cTnI&#39;] = covid[&#39;cTnI&#39;].astype(np.float64) covid[&#39;NTproBNP&#39;] = covid[&#39;NTproBNP&#39;].astype(np.float64) covid[&#39;Cr&#39;] = covid[&#39;Cr&#39;].astype(np.float64) . 2. Train Test Split . from sklearn import preprocessing from sklearn.model_selection import train_test_split . y = covid.Severity01.to_numpy() . # Use Both # covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;], axis=1) . # Use AIVolume # covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;, &quot;CTScore&quot;], axis=1) . # Use CTScore # covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;, &quot;AIVolumeP&quot;], axis=1) . # Use None covid = covid.drop([&quot;Severity01&quot;, &quot;Severity03&quot;, &quot;CTScore&quot;, &quot;AIVolumeP&quot;], axis=1) . X = covid X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.9, random_state = 1) . X_train.shape, X_test.shape . ((77, 56), (9, 56)) . 3. Feature Selection . 3.1 Basic Methods . 3.1.1 Drop constant and Quasi-constant features . Maybe those outliers are severe patients . from sklearn.feature_selection import VarianceThreshold . def drop_features(X_train, X_test, threshhold): sel = VarianceThreshold(threshold=threshhold) sel.fit(X_train) print(&quot;No. of constant features:&quot;, len([ x for x in X_train.columns if x not in X_train.columns[sel.get_support()] ]) ) constant_features = [x for x in X_train.columns if x not in X_train.columns[sel.get_support()]] print(constant_features) X_train.drop(labels=constant_features, axis=1, inplace=True) X_test.drop(labels=constant_features, axis=1, inplace=True) . Drop constant and quasi-constant features . drop_features(X_train, X_test, 0.01) . No. of constant features: 2 [&#39;PCT2&#39;, &#39;Stomachache&#39;] . d: anaconda3 envs alibi lib site-packages pandas core frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 54), (9, 54)) . 3.1.2 Drop Duplicated Features . Maybe some symptoms are correlated . covid_t = covid.T print(&quot;No. of Duplicated Features:&quot;, covid_t.duplicated().sum()) print(covid_t[covid_t.duplicated()].index.values) . No. of Duplicated Features: 1 [&#39;Arrythmia&#39;] . Print out duplicated features . duplicated_feat = [] for i in range(0, len(X_train.columns)): col_1 = X_train.columns[i] for col_2 in X_train.columns[i + 1 : ]: if X_train[col_1].equals(X_train[col_2]): print(col_1) print(col_2) duplicated_feat.append(col_2) . CAD Arrythmia . Drop duplicated features . # covid_unique = covid_t.drop_duplicates(keep=&#39;first&#39;).T . X_train.drop(labels=covid_t[covid_t.duplicated()].index.values, axis=1, inplace=True) X_test.drop(labels=covid_t[covid_t.duplicated()].index.values, axis=1, inplace=True) . X_train.shape, X_test.shape . ((77, 53), (9, 53)) . 3.2 Correlations . categorical_features = [&#39;Sex&#39;, &#39;AgeG1&#39;, &#39;Fever&#39;, &#39;Cough&#39;, &#39;Phlegm&#39;, &#39;Hemoptysis&#39;, &#39;SoreThroat&#39;, &#39;Catarrh&#39;, &#39;Headache&#39;, &#39;ChestPain&#39;, &#39;Fatigue&#39;, &#39;SoreMuscle&#39;, # &#39;Stomachache&#39;, &#39;Diarrhea&#39;, &#39;PoorAppetite&#39;, &#39;NauseaNVomit&#39;, &#39;Hypertention&#39;, &#39;Hyperlipedia&#39;, &#39;DM&#39;, &#39;Lung&#39;, #&#39;CAD&#39;, &#39;Arrythmia&#39;, &#39;Cancer&#39;] . numerical_features = [&#39;Age&#39;, &#39;Height&#39;, &#39;Weight&#39;, &#39;BMI&#39;, &#39;Temp&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;AST&#39;, &#39;LDH&#39;, &#39;CK&#39;, &#39;CKMB&#39;, &#39;HBDH&#39;, &#39;HiCKMB&#39;, &#39;NTproBNP&#39;, &#39;Cr&#39;, &#39;PCT1&#39;, &#39;WBC1&#39;, &#39;NEU1&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;ALB1&#39;, &#39;WBC2&#39;, &#39;NEU2&#39;, &#39;LYM2&#39;, &#39;N2L2&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;] . # numerics = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;] # numerical_vars = list(covid.select_dtypes(include=numerics).columns) # data = covid[numerical_vars] . corrmat = X_train.corr() . fig, ax = plt.subplots() fig.set_size_inches(11, 11) sns.heatmap(corrmat) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12048ba8&gt; . corrmat = X_train.corr() corrmat = corrmat.abs().unstack() corrmat = corrmat.sort_values(ascending=False) corrmat = corrmat[corrmat &gt;= 0.8] corrmat = corrmat[corrmat &lt; 1] corrmat = pd.DataFrame(corrmat).reset_index() corrmat.columns = [&#39;feature1&#39;, &#39;feature2&#39;, &#39;corr&#39;] corrmat . feature1 feature2 corr . 0 HBDH | LDH | 0.958191 | . 1 LDH | HBDH | 0.958191 | . 2 Height | PoorAppetite | 0.911704 | . 3 PoorAppetite | Height | 0.911704 | . 4 WBC2 | NEU2 | 0.911419 | . 5 NEU2 | WBC2 | 0.911419 | . 6 NEU1 | WBC1 | 0.903520 | . 7 WBC1 | NEU1 | 0.903520 | . 8 Age | AgeG1 | 0.893413 | . 9 AgeG1 | Age | 0.893413 | . 10 cTnICKMBOrdinal2 | cTnICKMBOrdinal1 | 0.853741 | . 11 cTnICKMBOrdinal1 | cTnICKMBOrdinal2 | 0.853741 | . 12 LYM1 | LYM2 | 0.842688 | . 13 LYM2 | LYM1 | 0.842688 | . 14 BMI | Weight | 0.842409 | . 15 Weight | BMI | 0.842409 | . 16 N2L2 | NTproBNP | 0.808767 | . 17 NTproBNP | N2L2 | 0.808767 | . # find groups of correlated features grouped_feature_ls = [] correlated_groups = [] for feature in corrmat.feature1.unique(): if feature not in grouped_feature_ls: # find all features correlated to a single feature correlated_block = corrmat[corrmat.feature1 == feature] grouped_feature_ls = grouped_feature_ls + list( correlated_block.feature2.unique()) + [feature] # append the block of features to the list correlated_groups.append(correlated_block) print(&#39;found {} correlated groups&#39;.format(len(correlated_groups))) print(&#39;out of {} total features&#39;.format(X_train.shape[1])) . found 9 correlated groups out of 53 total features . # now we can visualise each group. We see that some groups contain # only 2 correlated features, some other groups present several features # that are correlated among themselves. for group in correlated_groups: print(group) print() . feature1 feature2 corr 0 HBDH LDH 0.958191 feature1 feature2 corr 2 Height PoorAppetite 0.911704 feature1 feature2 corr 4 WBC2 NEU2 0.911419 feature1 feature2 corr 6 NEU1 WBC1 0.90352 feature1 feature2 corr 8 Age AgeG1 0.893413 feature1 feature2 corr 10 cTnICKMBOrdinal2 cTnICKMBOrdinal1 0.853741 feature1 feature2 corr 12 LYM1 LYM2 0.842688 feature1 feature2 corr 14 BMI Weight 0.842409 feature1 feature2 corr 16 N2L2 NTproBNP 0.808767 . def correlation(dataset, threshold): col_corr = set() corr_matrix = dataset.corr() for i in range(len(corr_matrix.columns)): for j in range(i): if abs(corr_matrix.iloc[i, j] &gt;= threshold): colname = corr_matrix.columns[i] col_corr.add(colname) return col_corr . corr_features = list((correlation(X_train, 0.8))) print(corr_features) . [&#39;LYM2&#39;, &#39;N2L2&#39;, &#39;AgeG1&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;NEU1&#39;, &#39;BMI&#39;, &#39;NEU2&#39;, &#39;HBDH&#39;] . for i in corr_features: if i in categorical_features: corr_features.remove(i) . corr_features . [&#39;LYM2&#39;, &#39;N2L2&#39;, &#39;cTnICKMBOrdinal2&#39;, &#39;NEU1&#39;, &#39;BMI&#39;, &#39;NEU2&#39;, &#39;HBDH&#39;] . for i in corr_features: if i in numerical_features: numerical_features.remove(i) for i in corr_features: if i in categorical_features: categorical_features.remove(i) . X_train.drop(labels=corr_features, axis=1, inplace=True) X_test.drop(labels=corr_features, axis=1, inplace=True) . d: anaconda3 envs alibi lib site-packages pandas core frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 46), (9, 46)) . 3.3 Statistical Methods . 3.3.1 Mutual Information . from sklearn.feature_selection import mutual_info_classif, mutual_info_regression from sklearn.feature_selection import SelectKBest, SelectPercentile . mi = mutual_info_classif(X_train, y_train) mi = pd.Series(mi) mi.index = X_train.columns . Features on the left side have more mutual information with y . mi.sort_values(ascending=False).plot.bar(figsize=(20, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x124b31d0&gt; . sel_ = SelectKBest(mutual_info_classif, k = 40).fit(X_train, y_train) . mi_features = list(X_train.columns[ ~ sel_.get_support()].values) . for i in mi_features: if i in categorical_features: mi_features.remove(i) . mi_features . [&#39;Height&#39;, &#39;Temp&#39;, &#39;AST&#39;, &#39;CK&#39;, &#39;HiCKMB&#39;] . for i in mi_features: if i in numerical_features: numerical_features.remove(i) for i in mi_features: if i in categorical_features: categorical_features.remove(i) . X_train.drop(labels=mi_features, axis=1, inplace=True) X_test.drop(labels=mi_features, axis=1, inplace=True) . d: anaconda3 envs alibi lib site-packages pandas core frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 41), (9, 41)) . 3.3.2 Fisher Score . from sklearn.feature_selection import chi2 . categorical features . f_score = chi2(X_train[categorical_features], y_train) . The smaller ones have more correlations . p_values = pd.Series(f_score[1]) p_values.index = X_train[categorical_features].columns p_values.sort_values(ascending=False) . Cancer 0.887949 SoreThroat 0.842057 Cough 0.703238 Headache 0.638344 Hemoptysis 0.594525 ChestPain 0.356552 NauseaNVomit 0.356552 Diarrhea 0.333947 Sex 0.302537 Fever 0.202574 Catarrh 0.159040 Hypertention 0.154388 SoreMuscle 0.105717 Hyperlipedia 0.099153 Lung 0.062605 PoorAppetite 0.060289 Phlegm 0.046410 AgeG1 0.037459 DM 0.008457 Fatigue 0.000049 dtype: float64 . p_values[p_values&lt;0.05].index.values . array([&#39;AgeG1&#39;, &#39;Phlegm&#39;, &#39;Fatigue&#39;, &#39;DM&#39;], dtype=object) . for c in categorical_features: if c not in p_values[p_values&lt;0.05].index.values: categorical_features.remove(c) X_train.drop(labels=c, axis=1, inplace=True) X_test.drop(labels=c, axis=1, inplace=True) . d: anaconda3 envs alibi lib site-packages pandas core frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 32), (9, 32)) . 3.3.3 Univariate . Non-categorical features . from sklearn.feature_selection import f_classif, f_regression . univariate = f_classif(X_train[numerical_features], y_train) univariate = pd.Series(univariate[1]) univariate.index = X_train[numerical_features].columns univariate.sort_values(ascending=False, inplace=True) . univariate.sort_values(ascending=False).plot.bar(figsize=(20, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1539af28&gt; . univariate[univariate &gt; 0.05] . PCT1 0.802511 WBC1 0.430778 CKMB 0.351257 WBC2 0.207957 Weight 0.191531 Cr 0.062507 dtype: float64 . for n in numerical_features: if n in univariate[univariate &gt; 0.05].index.values: numerical_features.remove(n) X_train.drop(labels=n, axis=1, inplace=True) X_test.drop(labels=n, axis=1, inplace=True) . d: anaconda3 envs alibi lib site-packages pandas core frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . univariate[univariate &gt; 0.05] . PCT1 0.802511 WBC1 0.430778 CKMB 0.351257 WBC2 0.207957 Weight 0.191531 Cr 0.062507 dtype: float64 . X_train.shape . (77, 27) . X_test.shape . (9, 27) . 3.3.4 ROC-AUC . from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.metrics import roc_auc_score, mean_squared_error . # loop to build a tree, make predictions and get the roc-auc # for each feature of the train set roc_values = [] for feature in X_train.columns: clf = DecisionTreeClassifier() clf.fit(X_train[feature].fillna(0).to_frame(), y_train) y_scored = clf.predict_proba(X_test[feature].fillna(0).to_frame()) roc_values.append(roc_auc_score(y_test, y_scored[:, 1])) . # let&#39;s add the variable names and order it for clearer visualisation roc_values = pd.Series(roc_values) roc_values.index = X_train.columns roc_values.sort_values(ascending=False) . cTnITimes 0.775 CRP2 0.675 SoreThroat 0.650 NTproBNP 0.650 LYM1 0.625 N2L1 0.625 ALB2 0.625 No 0.625 Headache 0.625 PCT1 0.550 AgeG1 0.550 Diarrhea 0.500 Age 0.500 cTnI 0.500 cTnICKMBOrdinal1 0.500 Phlegm 0.500 NauseaNVomit 0.500 CRP1 0.500 CAD 0.500 Fatigue 0.500 LDH 0.500 ALB1 0.450 Hyperlipedia 0.400 DM 0.400 Cancer 0.400 Sympton 0.375 Cough 0.325 dtype: float64 . # and now let&#39;s plot roc_values.sort_values(ascending=False).plot.bar(figsize=(20, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1511f7b8&gt; . # a roc auc value of 0.5 indicates random decision # let&#39;s check how many features show a roc-auc value # higher than random len(roc_values[roc_values &gt; 0.5]) . 11 . roc_values[roc_values &lt; 0.5] . LDH 0.500 ALB1 0.450 Sympton 0.375 Cough 0.325 Hyperlipedia 0.400 DM 0.400 Cancer 0.400 dtype: float64 . roc_features = roc_values[roc_values &lt; 0.5].index.values . for i in roc_features: if i in numerical_features: numerical_features.remove(i) for i in roc_features: if i in categorical_features: categorical_features.remove(i) . X_train.drop(labels=roc_features, axis=1, inplace=True) X_test.drop(labels=roc_features, axis=1, inplace=True) . d: anaconda3 envs alibi lib site-packages pandas core frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . X_train.shape, X_test.shape . ((77, 20), (9, 20)) . 4. Feature Engineering . Add New Features . covid[&#39;NSympton&#39;] = covid[&#39;Fever&#39;] + covid[&#39;Cough&#39;] + covid[&#39;Phlegm&#39;] + covid[&#39;Hemoptysis&#39;] + covid[&#39;SoreThroat&#39;] + covid[&#39;Catarrh&#39;] + covid[&#39;Headache&#39;] + covid[&#39;ChestPain&#39;] + covid[&#39;Fatigue&#39;] + covid[&#39;SoreMuscle&#39;]+covid[&#39;Stomachache&#39;] + covid[&#39;Diarrhea&#39;] + covid[&#39;PoorAppetite&#39;] + covid[&#39;NauseaNVomit&#39;] . covid[&#39;NDisease&#39;] = covid[&#39;Hypertention&#39;] + covid[&#39;Hyperlipedia&#39;] + covid[&#39;DM&#39;] + covid[&#39;Lung&#39;] + covid[&#39;CAD&#39;] + covid[&#39;Arrythmia&#39;] + covid[&#39;Cancer&#39;] . Feature Tools . #collapse-hide # import featuretools as ft . . #collapse-hide # es = ft.EntitySet(id=&quot;covid&quot;) # es = es.entity_from_dataframe(entity_id=&quot;covid&quot;, # dataframe=covid, # index=&quot;No&quot;,) . . #collapse-hide # feature_matrix, feature_defs = ft.dfs(entityset=es, # target_entity=&quot;covid&quot;, # trans_primitives=[&#39;time_since&#39;, &#39;day&#39;, &#39;is_weekend&#39;, # &#39;cum_min&#39;, &#39;minute&#39;, # &#39;num_words&#39;, &#39;weekday&#39;, &#39;cum_count&#39;, # &#39;percentile&#39;, &#39;year&#39;, &#39;week&#39;, # &#39;cum_mean&#39;]) # covid = feature_matrix # feature_names = covid.columns . . #collapse-hide # categorical_features = [1, 3, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56] # categorical_names = {} # for c in categorical_features: # categorical_names[c] = [&quot;False&quot;, &quot;True&quot;] . . #collapse-hide # for i,j in enumerate(feature_names): # print(i, j) . . X_train.shape, X_test.shape . ((77, 20), (9, 20)) . X_train[&#39;Age&#39;].min() . 15 . 5. Classifier . import sklearn import sklearn.ensemble import sklearn.metrics import xgboost as xgb . Cross Validation . from sklearn.model_selection import cross_val_score . def cv_score(classifier, X, y, scoring): return cross_val_score(classifier, X, y, cv=5, scoring=scoring) . Decision Tree . dt = sklearn.tree.DecisionTreeClassifier() dt_f1 = cv_score(dt, X_train, y_train, &#39;f1&#39;) dt.fit(X_train, y_train) dt_pred = dt.predict(X_test) . from sklearn import tree . tree.plot_tree(dt) . [Text(209.25, 199.32, &#39;X[6] &lt;= 1140.0 ngini = 0.344 nsamples = 77 nvalue = [60, 17]&#39;), Text(167.4, 163.07999999999998, &#39;X[11] &lt;= 14.615 ngini = 0.187 nsamples = 67 nvalue = [60, 7]&#39;), Text(125.55000000000001, 126.83999999999999, &#39;gini = 0.0 nsamples = 45 nvalue = [45, 0]&#39;), Text(209.25, 126.83999999999999, &#39;X[0] &lt;= 71.5 ngini = 0.434 nsamples = 22 nvalue = [15, 7]&#39;), Text(167.4, 90.6, &#39;X[4] &lt;= 0.012 ngini = 0.332 nsamples = 19 nvalue = [15, 4]&#39;), Text(83.7, 54.359999999999985, &#39;X[6] &lt;= 871.0 ngini = 0.133 nsamples = 14 nvalue = [13, 1]&#39;), Text(41.85, 18.119999999999976, &#39;gini = 0.0 nsamples = 13 nvalue = [13, 0]&#39;), Text(125.55000000000001, 18.119999999999976, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 1]&#39;), Text(251.10000000000002, 54.359999999999985, &#39;X[9] &lt;= 1.575 ngini = 0.48 nsamples = 5 nvalue = [2, 3]&#39;), Text(209.25, 18.119999999999976, &#39;gini = 0.0 nsamples = 2 nvalue = [2, 0]&#39;), Text(292.95, 18.119999999999976, &#39;gini = 0.0 nsamples = 3 nvalue = [0, 3]&#39;), Text(251.10000000000002, 90.6, &#39;gini = 0.0 nsamples = 3 nvalue = [0, 3]&#39;), Text(251.10000000000002, 163.07999999999998, &#39;gini = 0.0 nsamples = 10 nvalue = [0, 10]&#39;)] . tree.export_graphviz(dt, out_file=&quot;tree.dot&quot;, feature_names = X_train.columns, class_names=[&#39;Normal&#39;, &#39;Severe&#39;], filled = True) . Random Forest . from sklearn.ensemble import RandomForestClassifier . rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100) rf_f1 = cv_score(rf, X_train, y_train, &#39;f1&#39;) rf.fit(X_train, y_train) rf_pred = rf.predict(X_test) . # we get the feature importance attributed by the # random forest model (more on this in coming lectures) importance = pd.concat( [pd.Series(X_train.columns), pd.Series(rf.feature_importances_)], axis=1) importance.columns = [&#39;feature&#39;, &#39;importance&#39;] importance.sort_values(by=&#39;importance&#39;, ascending=False) . feature importance . 6 NTproBNP | 0.213304 | . 11 CRP2 | 0.162438 | . 12 ALB2 | 0.097181 | . 3 cTnITimes | 0.092945 | . 4 cTnI | 0.077726 | . 10 CRP1 | 0.074246 | . 0 No | 0.055817 | . 1 Age | 0.051989 | . 9 N2L1 | 0.039079 | . 8 LYM1 | 0.036814 | . 16 Fatigue | 0.032316 | . 7 PCT1 | 0.019300 | . 5 cTnICKMBOrdinal1 | 0.011147 | . 2 AgeG1 | 0.009732 | . 14 SoreThroat | 0.008761 | . 13 Phlegm | 0.007844 | . 17 Diarrhea | 0.007606 | . 15 Headache | 0.001147 | . 19 CAD | 0.000483 | . 18 NauseaNVomit | 0.000126 | . XGBoost . # Create a model # Params from: https://www.kaggle.com/aharless/swetha-s-xgboost-revised xgbc = xgb.XGBClassifier( max_depth = 4, subsample = 0.8, colsample_bytree = 0.7, colsample_bylevel = 0.7, scale_pos_weight = 9, min_child_weight = 0, reg_alpha = 4, objective = &#39;binary:logistic&#39; ) xgbc_f1 = cv_score(xgbc, X_train, y_train, &#39;f1&#39;) # Fit the models xgbc.fit(X_train.to_numpy(), y_train) xgbc_pred = xgbc.predict(X_test.to_numpy()) . Neural Networks . import keras from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout . Using TensorFlow backend. . from keras.wrappers.scikit_learn import KerasClassifier from sklearn.model_selection import cross_val_score . def build_classifier() : nn = Sequential() nn.add(Dense(activation=&#39;relu&#39;, input_dim=X_train.shape[1], units=10)) nn.add(Dropout(rate = 0.1)) nn.add(Dense(kernel_initializer=&quot;uniform&quot;, activation=&#39;relu&#39;, units=15)) nn.add(Dropout(rate = 0.1)) nn.add(Dense(kernel_initializer=&#39;uniform&#39;,activation=&#39;sigmoid&#39;, units=1)) nn.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return nn . #regressor = KerasRegressor(build_fn=build_regressor, epochs = 5000, batch_size= 100) nn = build_classifier(); . WARNING:tensorflow:From d: anaconda3 envs alibi lib site-packages tensorflow_core python ops resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From d: anaconda3 envs alibi lib site-packages tensorflow_core python ops nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . history = nn.fit(X_train, y_train, batch_size=100, epochs=500, validation_split = 0.2) #verbose = 2 . WARNING:tensorflow:From d: anaconda3 envs alibi lib site-packages keras backend tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. Train on 61 samples, validate on 16 samples Epoch 1/500 61/61 [==============================] - 0s 5ms/step - loss: 0.8465 - accuracy: 0.6885 - val_loss: 0.8094 - val_accuracy: 0.6875 Epoch 2/500 61/61 [==============================] - 0s 256us/step - loss: 0.7849 - accuracy: 0.7377 - val_loss: 0.7534 - val_accuracy: 0.6250 Epoch 3/500 61/61 [==============================] - 0s 0us/step - loss: 0.7577 - accuracy: 0.6885 - val_loss: 0.7037 - val_accuracy: 0.5000 Epoch 4/500 61/61 [==============================] - 0s 305us/step - loss: 0.8107 - accuracy: 0.6557 - val_loss: 0.6654 - val_accuracy: 0.5000 Epoch 5/500 61/61 [==============================] - 0s 82us/step - loss: 0.6618 - accuracy: 0.7213 - val_loss: 0.6401 - val_accuracy: 0.5000 Epoch 6/500 61/61 [==============================] - 0s 82us/step - loss: 0.6637 - accuracy: 0.6721 - val_loss: 0.6247 - val_accuracy: 0.5000 Epoch 7/500 61/61 [==============================] - 0s 82us/step - loss: 0.6346 - accuracy: 0.6393 - val_loss: 0.6172 - val_accuracy: 0.5000 Epoch 8/500 61/61 [==============================] - 0s 82us/step - loss: 0.6595 - accuracy: 0.6066 - val_loss: 0.6136 - val_accuracy: 0.5000 Epoch 9/500 61/61 [==============================] - 0s 82us/step - loss: 0.6608 - accuracy: 0.5738 - val_loss: 0.6122 - val_accuracy: 0.5000 Epoch 10/500 61/61 [==============================] - 0s 82us/step - loss: 0.6500 - accuracy: 0.6393 - val_loss: 0.6115 - val_accuracy: 0.5625 Epoch 11/500 61/61 [==============================] - 0s 82us/step - loss: 0.6454 - accuracy: 0.5902 - val_loss: 0.6105 - val_accuracy: 0.5625 Epoch 12/500 61/61 [==============================] - 0s 98us/step - loss: 0.6504 - accuracy: 0.6885 - val_loss: 0.6089 - val_accuracy: 0.5625 Epoch 13/500 61/61 [==============================] - 0s 82us/step - loss: 0.6465 - accuracy: 0.6721 - val_loss: 0.6063 - val_accuracy: 0.5625 Epoch 14/500 61/61 [==============================] - 0s 82us/step - loss: 0.6208 - accuracy: 0.6557 - val_loss: 0.6029 - val_accuracy: 0.6250 Epoch 15/500 61/61 [==============================] - 0s 82us/step - loss: 0.6333 - accuracy: 0.7541 - val_loss: 0.5985 - val_accuracy: 0.6250 Epoch 16/500 61/61 [==============================] - 0s 82us/step - loss: 0.6200 - accuracy: 0.7541 - val_loss: 0.5935 - val_accuracy: 0.6250 Epoch 17/500 61/61 [==============================] - 0s 148us/step - loss: 0.6045 - accuracy: 0.7377 - val_loss: 0.5877 - val_accuracy: 0.6250 Epoch 18/500 61/61 [==============================] - 0s 66us/step - loss: 0.6328 - accuracy: 0.7377 - val_loss: 0.5816 - val_accuracy: 0.6250 Epoch 19/500 61/61 [==============================] - 0s 66us/step - loss: 0.6249 - accuracy: 0.7705 - val_loss: 0.5743 - val_accuracy: 0.6250 Epoch 20/500 61/61 [==============================] - 0s 66us/step - loss: 0.6186 - accuracy: 0.8197 - val_loss: 0.5664 - val_accuracy: 0.6875 Epoch 21/500 61/61 [==============================] - 0s 82us/step - loss: 0.5928 - accuracy: 0.8361 - val_loss: 0.5586 - val_accuracy: 0.7500 Epoch 22/500 61/61 [==============================] - 0s 82us/step - loss: 0.6164 - accuracy: 0.8033 - val_loss: 0.5513 - val_accuracy: 0.7500 Epoch 23/500 61/61 [==============================] - 0s 82us/step - loss: 0.5992 - accuracy: 0.8361 - val_loss: 0.5441 - val_accuracy: 0.7500 Epoch 24/500 61/61 [==============================] - 0s 131us/step - loss: 0.5866 - accuracy: 0.8525 - val_loss: 0.5371 - val_accuracy: 0.7500 Epoch 25/500 61/61 [==============================] - 0s 66us/step - loss: 0.5687 - accuracy: 0.8033 - val_loss: 0.5310 - val_accuracy: 0.8750 Epoch 26/500 61/61 [==============================] - 0s 98us/step - loss: 0.5997 - accuracy: 0.7869 - val_loss: 0.5252 - val_accuracy: 0.8750 Epoch 27/500 61/61 [==============================] - 0s 66us/step - loss: 0.5813 - accuracy: 0.8525 - val_loss: 0.5192 - val_accuracy: 0.9375 Epoch 28/500 61/61 [==============================] - 0s 82us/step - loss: 0.5746 - accuracy: 0.8361 - val_loss: 0.5129 - val_accuracy: 0.9375 Epoch 29/500 61/61 [==============================] - 0s 82us/step - loss: 0.5976 - accuracy: 0.8525 - val_loss: 0.5067 - val_accuracy: 0.9375 Epoch 30/500 61/61 [==============================] - 0s 82us/step - loss: 0.5440 - accuracy: 0.8197 - val_loss: 0.5009 - val_accuracy: 0.9375 Epoch 31/500 61/61 [==============================] - 0s 66us/step - loss: 0.5544 - accuracy: 0.8525 - val_loss: 0.4955 - val_accuracy: 0.9375 Epoch 32/500 61/61 [==============================] - 0s 115us/step - loss: 0.5676 - accuracy: 0.7869 - val_loss: 0.4918 - val_accuracy: 0.9375 Epoch 33/500 61/61 [==============================] - 0s 131us/step - loss: 0.5356 - accuracy: 0.8689 - val_loss: 0.4884 - val_accuracy: 0.9375 Epoch 34/500 61/61 [==============================] - 0s 98us/step - loss: 0.5519 - accuracy: 0.8361 - val_loss: 0.4848 - val_accuracy: 0.9375 Epoch 35/500 61/61 [==============================] - 0s 98us/step - loss: 0.5249 - accuracy: 0.8689 - val_loss: 0.4798 - val_accuracy: 0.9375 Epoch 36/500 61/61 [==============================] - 0s 82us/step - loss: 0.5135 - accuracy: 0.8852 - val_loss: 0.4741 - val_accuracy: 0.9375 Epoch 37/500 61/61 [==============================] - 0s 82us/step - loss: 0.5036 - accuracy: 0.8689 - val_loss: 0.4676 - val_accuracy: 0.9375 Epoch 38/500 61/61 [==============================] - 0s 82us/step - loss: 0.5221 - accuracy: 0.8689 - val_loss: 0.4604 - val_accuracy: 0.9375 Epoch 39/500 61/61 [==============================] - 0s 82us/step - loss: 0.5055 - accuracy: 0.8689 - val_loss: 0.4530 - val_accuracy: 0.9375 Epoch 40/500 61/61 [==============================] - 0s 66us/step - loss: 0.5220 - accuracy: 0.8525 - val_loss: 0.4456 - val_accuracy: 0.9375 Epoch 41/500 61/61 [==============================] - 0s 66us/step - loss: 0.5107 - accuracy: 0.8689 - val_loss: 0.4379 - val_accuracy: 0.9375 Epoch 42/500 61/61 [==============================] - 0s 131us/step - loss: 0.4925 - accuracy: 0.8689 - val_loss: 0.4297 - val_accuracy: 0.9375 Epoch 43/500 61/61 [==============================] - 0s 98us/step - loss: 0.4715 - accuracy: 0.8689 - val_loss: 0.4208 - val_accuracy: 0.9375 Epoch 44/500 61/61 [==============================] - 0s 82us/step - loss: 0.4889 - accuracy: 0.8689 - val_loss: 0.4115 - val_accuracy: 0.9375 Epoch 45/500 61/61 [==============================] - 0s 66us/step - loss: 0.4839 - accuracy: 0.8852 - val_loss: 0.4014 - val_accuracy: 0.9375 Epoch 46/500 61/61 [==============================] - 0s 82us/step - loss: 0.4607 - accuracy: 0.8689 - val_loss: 0.3917 - val_accuracy: 0.9375 Epoch 47/500 61/61 [==============================] - 0s 66us/step - loss: 0.4562 - accuracy: 0.8525 - val_loss: 0.3817 - val_accuracy: 0.9375 Epoch 48/500 61/61 [==============================] - 0s 82us/step - loss: 0.4599 - accuracy: 0.8525 - val_loss: 0.3709 - val_accuracy: 0.9375 Epoch 49/500 61/61 [==============================] - 0s 66us/step - loss: 0.4313 - accuracy: 0.8689 - val_loss: 0.3606 - val_accuracy: 0.9375 Epoch 50/500 61/61 [==============================] - 0s 82us/step - loss: 0.4466 - accuracy: 0.8689 - val_loss: 0.3505 - val_accuracy: 0.9375 Epoch 51/500 61/61 [==============================] - 0s 66us/step - loss: 0.4192 - accuracy: 0.8525 - val_loss: 0.3406 - val_accuracy: 0.9375 Epoch 52/500 61/61 [==============================] - 0s 148us/step - loss: 0.4262 - accuracy: 0.8525 - val_loss: 0.3310 - val_accuracy: 0.9375 Epoch 53/500 61/61 [==============================] - 0s 82us/step - loss: 0.4393 - accuracy: 0.8689 - val_loss: 0.3223 - val_accuracy: 0.9375 Epoch 54/500 61/61 [==============================] - 0s 66us/step - loss: 0.4007 - accuracy: 0.8852 - val_loss: 0.3140 - val_accuracy: 0.9375 Epoch 55/500 61/61 [==============================] - 0s 82us/step - loss: 0.4189 - accuracy: 0.8852 - val_loss: 0.3076 - val_accuracy: 0.9375 Epoch 56/500 61/61 [==============================] - 0s 66us/step - loss: 0.4134 - accuracy: 0.8689 - val_loss: 0.3016 - val_accuracy: 0.9375 Epoch 57/500 61/61 [==============================] - 0s 82us/step - loss: 0.4014 - accuracy: 0.8525 - val_loss: 0.2986 - val_accuracy: 0.9375 Epoch 58/500 61/61 [==============================] - 0s 66us/step - loss: 0.4035 - accuracy: 0.8689 - val_loss: 0.2984 - val_accuracy: 0.9375 Epoch 59/500 61/61 [==============================] - 0s 82us/step - loss: 0.3921 - accuracy: 0.8525 - val_loss: 0.2994 - val_accuracy: 0.9375 Epoch 60/500 61/61 [==============================] - 0s 82us/step - loss: 0.3945 - accuracy: 0.8361 - val_loss: 0.2986 - val_accuracy: 0.9375 Epoch 61/500 61/61 [==============================] - 0s 115us/step - loss: 0.4291 - accuracy: 0.8361 - val_loss: 0.2967 - val_accuracy: 0.9375 Epoch 62/500 61/61 [==============================] - 0s 98us/step - loss: 0.4439 - accuracy: 0.8361 - val_loss: 0.2972 - val_accuracy: 0.9375 Epoch 63/500 61/61 [==============================] - 0s 82us/step - loss: 0.3871 - accuracy: 0.8852 - val_loss: 0.2970 - val_accuracy: 0.9375 Epoch 64/500 61/61 [==============================] - 0s 82us/step - loss: 0.3963 - accuracy: 0.8525 - val_loss: 0.2958 - val_accuracy: 0.9375 Epoch 65/500 61/61 [==============================] - 0s 82us/step - loss: 0.4041 - accuracy: 0.8197 - val_loss: 0.2933 - val_accuracy: 0.9375 Epoch 66/500 61/61 [==============================] - 0s 82us/step - loss: 0.4158 - accuracy: 0.8525 - val_loss: 0.2904 - val_accuracy: 0.9375 Epoch 67/500 61/61 [==============================] - 0s 82us/step - loss: 0.4031 - accuracy: 0.8525 - val_loss: 0.2869 - val_accuracy: 0.9375 Epoch 68/500 61/61 [==============================] - 0s 82us/step - loss: 0.3640 - accuracy: 0.8361 - val_loss: 0.2823 - val_accuracy: 0.9375 Epoch 69/500 61/61 [==============================] - 0s 115us/step - loss: 0.4097 - accuracy: 0.8852 - val_loss: 0.2791 - val_accuracy: 0.9375 Epoch 70/500 61/61 [==============================] - 0s 164us/step - loss: 0.3942 - accuracy: 0.8689 - val_loss: 0.2756 - val_accuracy: 0.9375 Epoch 71/500 61/61 [==============================] - 0s 98us/step - loss: 0.3938 - accuracy: 0.8525 - val_loss: 0.2719 - val_accuracy: 0.9375 Epoch 72/500 61/61 [==============================] - 0s 98us/step - loss: 0.3676 - accuracy: 0.8361 - val_loss: 0.2671 - val_accuracy: 0.9375 Epoch 73/500 61/61 [==============================] - 0s 66us/step - loss: 0.3828 - accuracy: 0.8852 - val_loss: 0.2624 - val_accuracy: 0.9375 Epoch 74/500 61/61 [==============================] - 0s 82us/step - loss: 0.3417 - accuracy: 0.8689 - val_loss: 0.2574 - val_accuracy: 0.9375 Epoch 75/500 61/61 [==============================] - 0s 66us/step - loss: 0.4094 - accuracy: 0.8525 - val_loss: 0.2527 - val_accuracy: 0.9375 Epoch 76/500 61/61 [==============================] - 0s 82us/step - loss: 0.3814 - accuracy: 0.8689 - val_loss: 0.2484 - val_accuracy: 0.9375 Epoch 77/500 61/61 [==============================] - 0s 98us/step - loss: 0.3517 - accuracy: 0.8689 - val_loss: 0.2444 - val_accuracy: 0.9375 Epoch 78/500 61/61 [==============================] - 0s 82us/step - loss: 0.3203 - accuracy: 0.8689 - val_loss: 0.2408 - val_accuracy: 0.9375 Epoch 79/500 61/61 [==============================] - 0s 82us/step - loss: 0.3868 - accuracy: 0.8852 - val_loss: 0.2376 - val_accuracy: 0.9375 Epoch 80/500 61/61 [==============================] - 0s 98us/step - loss: 0.3603 - accuracy: 0.8689 - val_loss: 0.2343 - val_accuracy: 0.9375 Epoch 81/500 61/61 [==============================] - 0s 131us/step - loss: 0.3511 - accuracy: 0.8525 - val_loss: 0.2309 - val_accuracy: 0.9375 Epoch 82/500 61/61 [==============================] - 0s 98us/step - loss: 0.3696 - accuracy: 0.8689 - val_loss: 0.2289 - val_accuracy: 0.9375 Epoch 83/500 61/61 [==============================] - 0s 82us/step - loss: 0.3328 - accuracy: 0.8852 - val_loss: 0.2267 - val_accuracy: 0.9375 Epoch 84/500 61/61 [==============================] - 0s 98us/step - loss: 0.3652 - accuracy: 0.8852 - val_loss: 0.2255 - val_accuracy: 0.9375 Epoch 85/500 61/61 [==============================] - 0s 82us/step - loss: 0.3188 - accuracy: 0.8689 - val_loss: 0.2249 - val_accuracy: 0.9375 Epoch 86/500 61/61 [==============================] - 0s 82us/step - loss: 0.3447 - accuracy: 0.8852 - val_loss: 0.2250 - val_accuracy: 0.9375 Epoch 87/500 61/61 [==============================] - 0s 98us/step - loss: 0.3468 - accuracy: 0.9016 - val_loss: 0.2255 - val_accuracy: 0.9375 Epoch 88/500 61/61 [==============================] - 0s 98us/step - loss: 0.3307 - accuracy: 0.8852 - val_loss: 0.2259 - val_accuracy: 0.9375 Epoch 89/500 61/61 [==============================] - 0s 66us/step - loss: 0.3429 - accuracy: 0.9016 - val_loss: 0.2265 - val_accuracy: 0.9375 Epoch 90/500 61/61 [==============================] - 0s 82us/step - loss: 0.3346 - accuracy: 0.8852 - val_loss: 0.2278 - val_accuracy: 0.9375 Epoch 91/500 61/61 [==============================] - 0s 66us/step - loss: 0.3380 - accuracy: 0.8852 - val_loss: 0.2292 - val_accuracy: 0.9375 Epoch 92/500 61/61 [==============================] - 0s 82us/step - loss: 0.3566 - accuracy: 0.8689 - val_loss: 0.2305 - val_accuracy: 0.9375 Epoch 93/500 61/61 [==============================] - 0s 82us/step - loss: 0.3148 - accuracy: 0.8852 - val_loss: 0.2320 - val_accuracy: 0.9375 Epoch 94/500 61/61 [==============================] - 0s 66us/step - loss: 0.3143 - accuracy: 0.8689 - val_loss: 0.2331 - val_accuracy: 0.9375 Epoch 95/500 61/61 [==============================] - 0s 82us/step - loss: 0.3394 - accuracy: 0.8525 - val_loss: 0.2344 - val_accuracy: 0.9375 Epoch 96/500 61/61 [==============================] - 0s 82us/step - loss: 0.3597 - accuracy: 0.8689 - val_loss: 0.2355 - val_accuracy: 0.9375 Epoch 97/500 61/61 [==============================] - 0s 82us/step - loss: 0.3567 - accuracy: 0.8361 - val_loss: 0.2370 - val_accuracy: 0.9375 Epoch 98/500 61/61 [==============================] - 0s 82us/step - loss: 0.3503 - accuracy: 0.9016 - val_loss: 0.2385 - val_accuracy: 0.9375 Epoch 99/500 61/61 [==============================] - 0s 82us/step - loss: 0.3426 - accuracy: 0.8852 - val_loss: 0.2391 - val_accuracy: 0.9375 Epoch 100/500 61/61 [==============================] - 0s 82us/step - loss: 0.3342 - accuracy: 0.8852 - val_loss: 0.2390 - val_accuracy: 0.9375 Epoch 101/500 61/61 [==============================] - 0s 82us/step - loss: 0.3754 - accuracy: 0.8689 - val_loss: 0.2390 - val_accuracy: 0.9375 Epoch 102/500 61/61 [==============================] - 0s 82us/step - loss: 0.3429 - accuracy: 0.8525 - val_loss: 0.2391 - val_accuracy: 0.9375 Epoch 103/500 61/61 [==============================] - 0s 66us/step - loss: 0.3389 - accuracy: 0.8525 - val_loss: 0.2404 - val_accuracy: 0.9375 Epoch 104/500 61/61 [==============================] - 0s 82us/step - loss: 0.3270 - accuracy: 0.9180 - val_loss: 0.2409 - val_accuracy: 0.9375 Epoch 105/500 61/61 [==============================] - 0s 66us/step - loss: 0.3246 - accuracy: 0.8689 - val_loss: 0.2415 - val_accuracy: 0.9375 Epoch 106/500 61/61 [==============================] - 0s 98us/step - loss: 0.3102 - accuracy: 0.8689 - val_loss: 0.2414 - val_accuracy: 0.9375 Epoch 107/500 61/61 [==============================] - 0s 82us/step - loss: 0.3170 - accuracy: 0.8689 - val_loss: 0.2411 - val_accuracy: 0.9375 Epoch 108/500 61/61 [==============================] - 0s 98us/step - loss: 0.3005 - accuracy: 0.8525 - val_loss: 0.2407 - val_accuracy: 0.9375 Epoch 109/500 61/61 [==============================] - 0s 115us/step - loss: 0.3117 - accuracy: 0.8852 - val_loss: 0.2402 - val_accuracy: 0.9375 Epoch 110/500 61/61 [==============================] - 0s 82us/step - loss: 0.3055 - accuracy: 0.8852 - val_loss: 0.2395 - val_accuracy: 0.9375 Epoch 111/500 61/61 [==============================] - 0s 82us/step - loss: 0.3540 - accuracy: 0.9016 - val_loss: 0.2390 - val_accuracy: 0.9375 Epoch 112/500 61/61 [==============================] - 0s 82us/step - loss: 0.3473 - accuracy: 0.8689 - val_loss: 0.2386 - val_accuracy: 0.9375 Epoch 113/500 61/61 [==============================] - 0s 66us/step - loss: 0.3692 - accuracy: 0.8689 - val_loss: 0.2384 - val_accuracy: 0.9375 Epoch 114/500 61/61 [==============================] - 0s 82us/step - loss: 0.3478 - accuracy: 0.8689 - val_loss: 0.2388 - val_accuracy: 0.9375 Epoch 115/500 61/61 [==============================] - 0s 82us/step - loss: 0.3079 - accuracy: 0.8852 - val_loss: 0.2408 - val_accuracy: 0.9375 Epoch 116/500 61/61 [==============================] - 0s 82us/step - loss: 0.3360 - accuracy: 0.8689 - val_loss: 0.2422 - val_accuracy: 0.9375 Epoch 117/500 61/61 [==============================] - 0s 82us/step - loss: 0.2929 - accuracy: 0.8689 - val_loss: 0.2429 - val_accuracy: 0.9375 Epoch 118/500 61/61 [==============================] - 0s 98us/step - loss: 0.3221 - accuracy: 0.8689 - val_loss: 0.2429 - val_accuracy: 0.9375 Epoch 119/500 61/61 [==============================] - 0s 148us/step - loss: 0.3200 - accuracy: 0.8525 - val_loss: 0.2426 - val_accuracy: 0.9375 Epoch 120/500 61/61 [==============================] - 0s 82us/step - loss: 0.3666 - accuracy: 0.8689 - val_loss: 0.2447 - val_accuracy: 0.9375 Epoch 121/500 61/61 [==============================] - 0s 82us/step - loss: 0.3007 - accuracy: 0.8852 - val_loss: 0.2457 - val_accuracy: 0.9375 Epoch 122/500 61/61 [==============================] - 0s 98us/step - loss: 0.2867 - accuracy: 0.9016 - val_loss: 0.2458 - val_accuracy: 0.9375 Epoch 123/500 61/61 [==============================] - 0s 98us/step - loss: 0.3395 - accuracy: 0.8689 - val_loss: 0.2459 - val_accuracy: 0.9375 Epoch 124/500 61/61 [==============================] - 0s 66us/step - loss: 0.3296 - accuracy: 0.8852 - val_loss: 0.2466 - val_accuracy: 0.9375 Epoch 125/500 61/61 [==============================] - 0s 98us/step - loss: 0.3166 - accuracy: 0.8689 - val_loss: 0.2468 - val_accuracy: 0.9375 Epoch 126/500 61/61 [==============================] - 0s 98us/step - loss: 0.3171 - accuracy: 0.8689 - val_loss: 0.2462 - val_accuracy: 0.9375 Epoch 127/500 61/61 [==============================] - 0s 98us/step - loss: 0.3061 - accuracy: 0.8852 - val_loss: 0.2450 - val_accuracy: 0.9375 Epoch 128/500 61/61 [==============================] - 0s 82us/step - loss: 0.3138 - accuracy: 0.8525 - val_loss: 0.2436 - val_accuracy: 0.9375 Epoch 129/500 61/61 [==============================] - 0s 82us/step - loss: 0.2844 - accuracy: 0.8689 - val_loss: 0.2418 - val_accuracy: 0.9375 Epoch 130/500 61/61 [==============================] - 0s 66us/step - loss: 0.3027 - accuracy: 0.8361 - val_loss: 0.2401 - val_accuracy: 0.9375 Epoch 131/500 61/61 [==============================] - 0s 98us/step - loss: 0.3374 - accuracy: 0.8525 - val_loss: 0.2408 - val_accuracy: 0.9375 Epoch 132/500 61/61 [==============================] - 0s 82us/step - loss: 0.2840 - accuracy: 0.9180 - val_loss: 0.2417 - val_accuracy: 0.9375 Epoch 133/500 61/61 [==============================] - 0s 115us/step - loss: 0.2884 - accuracy: 0.8689 - val_loss: 0.2423 - val_accuracy: 0.9375 Epoch 134/500 61/61 [==============================] - 0s 66us/step - loss: 0.2931 - accuracy: 0.8852 - val_loss: 0.2422 - val_accuracy: 0.9375 Epoch 135/500 61/61 [==============================] - 0s 98us/step - loss: 0.2861 - accuracy: 0.8852 - val_loss: 0.2406 - val_accuracy: 0.9375 Epoch 136/500 61/61 [==============================] - 0s 82us/step - loss: 0.3249 - accuracy: 0.8525 - val_loss: 0.2391 - val_accuracy: 0.9375 Epoch 137/500 61/61 [==============================] - 0s 115us/step - loss: 0.3369 - accuracy: 0.8852 - val_loss: 0.2375 - val_accuracy: 0.9375 Epoch 138/500 61/61 [==============================] - 0s 82us/step - loss: 0.3436 - accuracy: 0.8852 - val_loss: 0.2375 - val_accuracy: 0.9375 Epoch 139/500 61/61 [==============================] - 0s 82us/step - loss: 0.3082 - accuracy: 0.8525 - val_loss: 0.2377 - val_accuracy: 0.9375 Epoch 140/500 61/61 [==============================] - 0s 115us/step - loss: 0.3281 - accuracy: 0.8689 - val_loss: 0.2401 - val_accuracy: 0.9375 Epoch 141/500 61/61 [==============================] - 0s 82us/step - loss: 0.3536 - accuracy: 0.8525 - val_loss: 0.2437 - val_accuracy: 0.9375 Epoch 142/500 61/61 [==============================] - 0s 82us/step - loss: 0.3198 - accuracy: 0.9016 - val_loss: 0.2468 - val_accuracy: 0.9375 Epoch 143/500 61/61 [==============================] - 0s 82us/step - loss: 0.2903 - accuracy: 0.8689 - val_loss: 0.2492 - val_accuracy: 0.9375 Epoch 144/500 61/61 [==============================] - 0s 82us/step - loss: 0.3141 - accuracy: 0.8525 - val_loss: 0.2498 - val_accuracy: 0.9375 Epoch 145/500 61/61 [==============================] - 0s 98us/step - loss: 0.2823 - accuracy: 0.8689 - val_loss: 0.2505 - val_accuracy: 0.9375 Epoch 146/500 61/61 [==============================] - 0s 82us/step - loss: 0.2928 - accuracy: 0.8852 - val_loss: 0.2504 - val_accuracy: 0.9375 Epoch 147/500 61/61 [==============================] - 0s 98us/step - loss: 0.3121 - accuracy: 0.8525 - val_loss: 0.2493 - val_accuracy: 0.9375 Epoch 148/500 61/61 [==============================] - 0s 82us/step - loss: 0.3677 - accuracy: 0.8361 - val_loss: 0.2496 - val_accuracy: 0.9375 Epoch 149/500 61/61 [==============================] - 0s 82us/step - loss: 0.2828 - accuracy: 0.8689 - val_loss: 0.2494 - val_accuracy: 0.9375 Epoch 150/500 61/61 [==============================] - 0s 82us/step - loss: 0.2683 - accuracy: 0.8852 - val_loss: 0.2500 - val_accuracy: 0.9375 Epoch 151/500 61/61 [==============================] - 0s 66us/step - loss: 0.2882 - accuracy: 0.8525 - val_loss: 0.2496 - val_accuracy: 0.9375 Epoch 152/500 61/61 [==============================] - 0s 82us/step - loss: 0.2998 - accuracy: 0.8689 - val_loss: 0.2495 - val_accuracy: 0.9375 Epoch 153/500 61/61 [==============================] - 0s 82us/step - loss: 0.2579 - accuracy: 0.9016 - val_loss: 0.2494 - val_accuracy: 0.9375 Epoch 154/500 61/61 [==============================] - 0s 115us/step - loss: 0.3010 - accuracy: 0.9016 - val_loss: 0.2487 - val_accuracy: 0.9375 Epoch 155/500 61/61 [==============================] - 0s 82us/step - loss: 0.3082 - accuracy: 0.8852 - val_loss: 0.2479 - val_accuracy: 0.9375 Epoch 156/500 61/61 [==============================] - 0s 82us/step - loss: 0.2908 - accuracy: 0.8689 - val_loss: 0.2474 - val_accuracy: 0.9375 Epoch 157/500 61/61 [==============================] - 0s 66us/step - loss: 0.2784 - accuracy: 0.8852 - val_loss: 0.2468 - val_accuracy: 0.9375 Epoch 158/500 61/61 [==============================] - 0s 82us/step - loss: 0.2885 - accuracy: 0.8852 - val_loss: 0.2464 - val_accuracy: 0.9375 Epoch 159/500 61/61 [==============================] - 0s 98us/step - loss: 0.2656 - accuracy: 0.9344 - val_loss: 0.2457 - val_accuracy: 0.9375 Epoch 160/500 61/61 [==============================] - 0s 82us/step - loss: 0.2922 - accuracy: 0.8361 - val_loss: 0.2439 - val_accuracy: 0.9375 Epoch 161/500 61/61 [==============================] - 0s 115us/step - loss: 0.2826 - accuracy: 0.9016 - val_loss: 0.2432 - val_accuracy: 0.9375 Epoch 162/500 61/61 [==============================] - 0s 98us/step - loss: 0.3169 - accuracy: 0.8361 - val_loss: 0.2435 - val_accuracy: 0.9375 Epoch 163/500 61/61 [==============================] - 0s 82us/step - loss: 0.2530 - accuracy: 0.9180 - val_loss: 0.2436 - val_accuracy: 0.9375 Epoch 164/500 61/61 [==============================] - 0s 66us/step - loss: 0.2529 - accuracy: 0.9016 - val_loss: 0.2437 - val_accuracy: 0.9375 Epoch 165/500 61/61 [==============================] - 0s 82us/step - loss: 0.2712 - accuracy: 0.9016 - val_loss: 0.2434 - val_accuracy: 0.9375 Epoch 166/500 61/61 [==============================] - 0s 82us/step - loss: 0.2824 - accuracy: 0.8852 - val_loss: 0.2432 - val_accuracy: 0.9375 Epoch 167/500 61/61 [==============================] - 0s 66us/step - loss: 0.2938 - accuracy: 0.8852 - val_loss: 0.2429 - val_accuracy: 0.9375 Epoch 168/500 61/61 [==============================] - 0s 82us/step - loss: 0.2851 - accuracy: 0.8852 - val_loss: 0.2426 - val_accuracy: 0.9375 Epoch 169/500 61/61 [==============================] - 0s 98us/step - loss: 0.2642 - accuracy: 0.8689 - val_loss: 0.2419 - val_accuracy: 0.9375 Epoch 170/500 61/61 [==============================] - 0s 115us/step - loss: 0.2947 - accuracy: 0.8852 - val_loss: 0.2422 - val_accuracy: 0.9375 Epoch 171/500 61/61 [==============================] - 0s 82us/step - loss: 0.2900 - accuracy: 0.8852 - val_loss: 0.2424 - val_accuracy: 0.9375 Epoch 172/500 61/61 [==============================] - 0s 82us/step - loss: 0.2752 - accuracy: 0.8852 - val_loss: 0.2427 - val_accuracy: 0.9375 Epoch 173/500 61/61 [==============================] - 0s 98us/step - loss: 0.2812 - accuracy: 0.8852 - val_loss: 0.2428 - val_accuracy: 0.9375 Epoch 174/500 61/61 [==============================] - 0s 82us/step - loss: 0.2736 - accuracy: 0.8852 - val_loss: 0.2439 - val_accuracy: 0.9375 Epoch 175/500 61/61 [==============================] - 0s 98us/step - loss: 0.2979 - accuracy: 0.9016 - val_loss: 0.2447 - val_accuracy: 0.8750 Epoch 176/500 61/61 [==============================] - 0s 98us/step - loss: 0.3081 - accuracy: 0.8525 - val_loss: 0.2461 - val_accuracy: 0.8750 Epoch 177/500 61/61 [==============================] - 0s 82us/step - loss: 0.2746 - accuracy: 0.9016 - val_loss: 0.2480 - val_accuracy: 0.8750 Epoch 178/500 61/61 [==============================] - 0s 66us/step - loss: 0.2876 - accuracy: 0.8525 - val_loss: 0.2498 - val_accuracy: 0.8750 Epoch 179/500 61/61 [==============================] - 0s 98us/step - loss: 0.2936 - accuracy: 0.8852 - val_loss: 0.2521 - val_accuracy: 0.8750 Epoch 180/500 61/61 [==============================] - 0s 82us/step - loss: 0.2643 - accuracy: 0.9016 - val_loss: 0.2533 - val_accuracy: 0.8750 Epoch 181/500 61/61 [==============================] - 0s 98us/step - loss: 0.2624 - accuracy: 0.9016 - val_loss: 0.2529 - val_accuracy: 0.8750 Epoch 182/500 61/61 [==============================] - 0s 82us/step - loss: 0.2402 - accuracy: 0.9180 - val_loss: 0.2511 - val_accuracy: 0.8750 Epoch 183/500 61/61 [==============================] - 0s 98us/step - loss: 0.2521 - accuracy: 0.8852 - val_loss: 0.2491 - val_accuracy: 0.8750 Epoch 184/500 61/61 [==============================] - 0s 98us/step - loss: 0.2803 - accuracy: 0.8852 - val_loss: 0.2456 - val_accuracy: 0.8750 Epoch 185/500 61/61 [==============================] - 0s 82us/step - loss: 0.2482 - accuracy: 0.8689 - val_loss: 0.2425 - val_accuracy: 0.8750 Epoch 186/500 61/61 [==============================] - 0s 82us/step - loss: 0.2911 - accuracy: 0.8525 - val_loss: 0.2415 - val_accuracy: 0.8750 Epoch 187/500 61/61 [==============================] - 0s 66us/step - loss: 0.2531 - accuracy: 0.8689 - val_loss: 0.2410 - val_accuracy: 0.8750 Epoch 188/500 61/61 [==============================] - 0s 66us/step - loss: 0.2956 - accuracy: 0.8852 - val_loss: 0.2422 - val_accuracy: 0.8750 Epoch 189/500 61/61 [==============================] - 0s 115us/step - loss: 0.2764 - accuracy: 0.8852 - val_loss: 0.2438 - val_accuracy: 0.8750 Epoch 190/500 61/61 [==============================] - 0s 82us/step - loss: 0.2626 - accuracy: 0.9344 - val_loss: 0.2460 - val_accuracy: 0.8750 Epoch 191/500 61/61 [==============================] - 0s 82us/step - loss: 0.2805 - accuracy: 0.8689 - val_loss: 0.2477 - val_accuracy: 0.8750 Epoch 192/500 61/61 [==============================] - 0s 66us/step - loss: 0.2370 - accuracy: 0.9016 - val_loss: 0.2479 - val_accuracy: 0.8750 Epoch 193/500 61/61 [==============================] - 0s 66us/step - loss: 0.2842 - accuracy: 0.8525 - val_loss: 0.2477 - val_accuracy: 0.8750 Epoch 194/500 61/61 [==============================] - 0s 164us/step - loss: 0.2735 - accuracy: 0.8852 - val_loss: 0.2475 - val_accuracy: 0.8750 Epoch 195/500 61/61 [==============================] - 0s 82us/step - loss: 0.2270 - accuracy: 0.9016 - val_loss: 0.2467 - val_accuracy: 0.8750 Epoch 196/500 61/61 [==============================] - 0s 82us/step - loss: 0.2959 - accuracy: 0.8689 - val_loss: 0.2474 - val_accuracy: 0.8750 Epoch 197/500 61/61 [==============================] - 0s 115us/step - loss: 0.2867 - accuracy: 0.9016 - val_loss: 0.2486 - val_accuracy: 0.8750 Epoch 198/500 61/61 [==============================] - 0s 66us/step - loss: 0.2237 - accuracy: 0.9180 - val_loss: 0.2491 - val_accuracy: 0.8750 Epoch 199/500 61/61 [==============================] - 0s 98us/step - loss: 0.2603 - accuracy: 0.9016 - val_loss: 0.2495 - val_accuracy: 0.8750 Epoch 200/500 61/61 [==============================] - 0s 98us/step - loss: 0.2829 - accuracy: 0.8689 - val_loss: 0.2517 - val_accuracy: 0.8750 Epoch 201/500 61/61 [==============================] - 0s 82us/step - loss: 0.2651 - accuracy: 0.8852 - val_loss: 0.2536 - val_accuracy: 0.8750 Epoch 202/500 61/61 [==============================] - 0s 82us/step - loss: 0.2407 - accuracy: 0.8689 - val_loss: 0.2549 - val_accuracy: 0.8750 Epoch 203/500 61/61 [==============================] - 0s 82us/step - loss: 0.2742 - accuracy: 0.8689 - val_loss: 0.2555 - val_accuracy: 0.8750 Epoch 204/500 61/61 [==============================] - 0s 66us/step - loss: 0.2546 - accuracy: 0.8525 - val_loss: 0.2551 - val_accuracy: 0.8750 Epoch 205/500 61/61 [==============================] - 0s 98us/step - loss: 0.2654 - accuracy: 0.8852 - val_loss: 0.2547 - val_accuracy: 0.8750 Epoch 206/500 61/61 [==============================] - 0s 66us/step - loss: 0.3241 - accuracy: 0.8525 - val_loss: 0.2543 - val_accuracy: 0.8750 Epoch 207/500 61/61 [==============================] - 0s 82us/step - loss: 0.2326 - accuracy: 0.9180 - val_loss: 0.2521 - val_accuracy: 0.8750 Epoch 208/500 61/61 [==============================] - 0s 82us/step - loss: 0.2720 - accuracy: 0.8689 - val_loss: 0.2489 - val_accuracy: 0.8750 Epoch 209/500 61/61 [==============================] - 0s 82us/step - loss: 0.2352 - accuracy: 0.8852 - val_loss: 0.2459 - val_accuracy: 0.8750 Epoch 210/500 61/61 [==============================] - 0s 98us/step - loss: 0.2695 - accuracy: 0.9016 - val_loss: 0.2434 - val_accuracy: 0.8750 Epoch 211/500 61/61 [==============================] - 0s 115us/step - loss: 0.2672 - accuracy: 0.8689 - val_loss: 0.2411 - val_accuracy: 0.8750 Epoch 212/500 61/61 [==============================] - 0s 98us/step - loss: 0.2482 - accuracy: 0.8852 - val_loss: 0.2384 - val_accuracy: 0.8750 Epoch 213/500 61/61 [==============================] - 0s 98us/step - loss: 0.2269 - accuracy: 0.8852 - val_loss: 0.2354 - val_accuracy: 0.8750 Epoch 214/500 61/61 [==============================] - 0s 98us/step - loss: 0.2579 - accuracy: 0.8689 - val_loss: 0.2334 - val_accuracy: 0.9375 Epoch 215/500 61/61 [==============================] - 0s 98us/step - loss: 0.2854 - accuracy: 0.8852 - val_loss: 0.2322 - val_accuracy: 0.9375 Epoch 216/500 61/61 [==============================] - 0s 82us/step - loss: 0.2516 - accuracy: 0.8689 - val_loss: 0.2319 - val_accuracy: 0.9375 Epoch 217/500 61/61 [==============================] - 0s 82us/step - loss: 0.2979 - accuracy: 0.8525 - val_loss: 0.2344 - val_accuracy: 0.8750 Epoch 218/500 61/61 [==============================] - 0s 98us/step - loss: 0.2296 - accuracy: 0.9016 - val_loss: 0.2373 - val_accuracy: 0.8750 Epoch 219/500 61/61 [==============================] - 0s 82us/step - loss: 0.2547 - accuracy: 0.8852 - val_loss: 0.2410 - val_accuracy: 0.8750 Epoch 220/500 61/61 [==============================] - 0s 98us/step - loss: 0.2402 - accuracy: 0.8852 - val_loss: 0.2452 - val_accuracy: 0.8750 Epoch 221/500 61/61 [==============================] - 0s 98us/step - loss: 0.2715 - accuracy: 0.8689 - val_loss: 0.2480 - val_accuracy: 0.8750 Epoch 222/500 61/61 [==============================] - 0s 82us/step - loss: 0.2540 - accuracy: 0.8852 - val_loss: 0.2569 - val_accuracy: 0.8750 Epoch 223/500 61/61 [==============================] - 0s 82us/step - loss: 0.2288 - accuracy: 0.9016 - val_loss: 0.2641 - val_accuracy: 0.8750 Epoch 224/500 61/61 [==============================] - 0s 82us/step - loss: 0.2577 - accuracy: 0.9016 - val_loss: 0.2687 - val_accuracy: 0.8750 Epoch 225/500 61/61 [==============================] - 0s 98us/step - loss: 0.2367 - accuracy: 0.9016 - val_loss: 0.2705 - val_accuracy: 0.8125 Epoch 226/500 61/61 [==============================] - 0s 82us/step - loss: 0.2294 - accuracy: 0.9016 - val_loss: 0.2699 - val_accuracy: 0.8125 Epoch 227/500 61/61 [==============================] - 0s 82us/step - loss: 0.2522 - accuracy: 0.8689 - val_loss: 0.2672 - val_accuracy: 0.8125 Epoch 228/500 61/61 [==============================] - 0s 82us/step - loss: 0.2372 - accuracy: 0.9016 - val_loss: 0.2632 - val_accuracy: 0.8125 Epoch 229/500 61/61 [==============================] - 0s 66us/step - loss: 0.2602 - accuracy: 0.8852 - val_loss: 0.2575 - val_accuracy: 0.8750 Epoch 230/500 61/61 [==============================] - 0s 115us/step - loss: 0.2310 - accuracy: 0.9016 - val_loss: 0.2518 - val_accuracy: 0.8750 Epoch 231/500 61/61 [==============================] - 0s 98us/step - loss: 0.2552 - accuracy: 0.8689 - val_loss: 0.2453 - val_accuracy: 0.8750 Epoch 232/500 61/61 [==============================] - 0s 66us/step - loss: 0.2549 - accuracy: 0.9016 - val_loss: 0.2388 - val_accuracy: 0.8750 Epoch 233/500 61/61 [==============================] - 0s 82us/step - loss: 0.2033 - accuracy: 0.9016 - val_loss: 0.2334 - val_accuracy: 0.8750 Epoch 234/500 61/61 [==============================] - 0s 98us/step - loss: 0.2947 - accuracy: 0.8525 - val_loss: 0.2302 - val_accuracy: 0.8750 Epoch 235/500 61/61 [==============================] - 0s 66us/step - loss: 0.2042 - accuracy: 0.8852 - val_loss: 0.2279 - val_accuracy: 0.8750 Epoch 236/500 61/61 [==============================] - 0s 82us/step - loss: 0.2608 - accuracy: 0.8852 - val_loss: 0.2270 - val_accuracy: 0.8750 Epoch 237/500 61/61 [==============================] - 0s 82us/step - loss: 0.2876 - accuracy: 0.8361 - val_loss: 0.2295 - val_accuracy: 0.8750 Epoch 238/500 61/61 [==============================] - 0s 82us/step - loss: 0.2606 - accuracy: 0.8525 - val_loss: 0.2335 - val_accuracy: 0.8750 Epoch 239/500 61/61 [==============================] - 0s 66us/step - loss: 0.2204 - accuracy: 0.8852 - val_loss: 0.2391 - val_accuracy: 0.8750 Epoch 240/500 61/61 [==============================] - 0s 98us/step - loss: 0.2475 - accuracy: 0.9180 - val_loss: 0.2426 - val_accuracy: 0.8750 Epoch 241/500 61/61 [==============================] - 0s 82us/step - loss: 0.2530 - accuracy: 0.9016 - val_loss: 0.2451 - val_accuracy: 0.8750 Epoch 242/500 61/61 [==============================] - 0s 82us/step - loss: 0.2383 - accuracy: 0.8852 - val_loss: 0.2474 - val_accuracy: 0.8750 Epoch 243/500 61/61 [==============================] - 0s 82us/step - loss: 0.2232 - accuracy: 0.9016 - val_loss: 0.2492 - val_accuracy: 0.8750 Epoch 244/500 61/61 [==============================] - 0s 66us/step - loss: 0.2539 - accuracy: 0.8852 - val_loss: 0.2529 - val_accuracy: 0.8125 Epoch 245/500 61/61 [==============================] - 0s 82us/step - loss: 0.2353 - accuracy: 0.8852 - val_loss: 0.2566 - val_accuracy: 0.8125 Epoch 246/500 61/61 [==============================] - 0s 82us/step - loss: 0.2366 - accuracy: 0.8689 - val_loss: 0.2586 - val_accuracy: 0.8125 Epoch 247/500 61/61 [==============================] - 0s 98us/step - loss: 0.2197 - accuracy: 0.9180 - val_loss: 0.2600 - val_accuracy: 0.8125 Epoch 248/500 61/61 [==============================] - 0s 82us/step - loss: 0.2252 - accuracy: 0.9180 - val_loss: 0.2591 - val_accuracy: 0.8125 Epoch 249/500 61/61 [==============================] - 0s 98us/step - loss: 0.2339 - accuracy: 0.9180 - val_loss: 0.2562 - val_accuracy: 0.8125 Epoch 250/500 61/61 [==============================] - 0s 66us/step - loss: 0.2248 - accuracy: 0.8852 - val_loss: 0.2514 - val_accuracy: 0.8125 Epoch 251/500 61/61 [==============================] - 0s 82us/step - loss: 0.2532 - accuracy: 0.8525 - val_loss: 0.2477 - val_accuracy: 0.8125 Epoch 252/500 61/61 [==============================] - 0s 82us/step - loss: 0.2246 - accuracy: 0.9016 - val_loss: 0.2433 - val_accuracy: 0.8750 Epoch 253/500 61/61 [==============================] - 0s 98us/step - loss: 0.2363 - accuracy: 0.9016 - val_loss: 0.2373 - val_accuracy: 0.8750 Epoch 254/500 61/61 [==============================] - 0s 98us/step - loss: 0.2956 - accuracy: 0.8852 - val_loss: 0.2352 - val_accuracy: 0.8750 Epoch 255/500 61/61 [==============================] - 0s 66us/step - loss: 0.2037 - accuracy: 0.8852 - val_loss: 0.2348 - val_accuracy: 0.8750 Epoch 256/500 61/61 [==============================] - 0s 98us/step - loss: 0.2093 - accuracy: 0.9016 - val_loss: 0.2344 - val_accuracy: 0.8750 Epoch 257/500 61/61 [==============================] - 0s 82us/step - loss: 0.1924 - accuracy: 0.9180 - val_loss: 0.2350 - val_accuracy: 0.8750 Epoch 258/500 61/61 [==============================] - 0s 82us/step - loss: 0.2767 - accuracy: 0.8361 - val_loss: 0.2371 - val_accuracy: 0.8750 Epoch 259/500 61/61 [==============================] - 0s 98us/step - loss: 0.2535 - accuracy: 0.8689 - val_loss: 0.2413 - val_accuracy: 0.8750 Epoch 260/500 61/61 [==============================] - 0s 98us/step - loss: 0.2001 - accuracy: 0.9180 - val_loss: 0.2434 - val_accuracy: 0.8125 Epoch 261/500 61/61 [==============================] - 0s 66us/step - loss: 0.2168 - accuracy: 0.9508 - val_loss: 0.2443 - val_accuracy: 0.8125 Epoch 262/500 61/61 [==============================] - 0s 82us/step - loss: 0.2130 - accuracy: 0.9180 - val_loss: 0.2443 - val_accuracy: 0.8125 Epoch 263/500 61/61 [==============================] - 0s 82us/step - loss: 0.2659 - accuracy: 0.9180 - val_loss: 0.2434 - val_accuracy: 0.8125 Epoch 264/500 61/61 [==============================] - 0s 82us/step - loss: 0.2463 - accuracy: 0.8852 - val_loss: 0.2453 - val_accuracy: 0.8125 Epoch 265/500 61/61 [==============================] - 0s 115us/step - loss: 0.2588 - accuracy: 0.8852 - val_loss: 0.2477 - val_accuracy: 0.8125 Epoch 266/500 61/61 [==============================] - 0s 82us/step - loss: 0.1951 - accuracy: 0.9180 - val_loss: 0.2500 - val_accuracy: 0.8125 Epoch 267/500 61/61 [==============================] - 0s 82us/step - loss: 0.2045 - accuracy: 0.9016 - val_loss: 0.2500 - val_accuracy: 0.8125 Epoch 268/500 61/61 [==============================] - 0s 66us/step - loss: 0.1938 - accuracy: 0.9016 - val_loss: 0.2488 - val_accuracy: 0.8125 Epoch 269/500 61/61 [==============================] - 0s 82us/step - loss: 0.2207 - accuracy: 0.9180 - val_loss: 0.2468 - val_accuracy: 0.8125 Epoch 270/500 61/61 [==============================] - 0s 66us/step - loss: 0.2084 - accuracy: 0.9016 - val_loss: 0.2453 - val_accuracy: 0.8125 Epoch 271/500 61/61 [==============================] - 0s 148us/step - loss: 0.2347 - accuracy: 0.8689 - val_loss: 0.2459 - val_accuracy: 0.8125 Epoch 272/500 61/61 [==============================] - 0s 66us/step - loss: 0.2626 - accuracy: 0.8852 - val_loss: 0.2476 - val_accuracy: 0.8125 Epoch 273/500 61/61 [==============================] - 0s 98us/step - loss: 0.2167 - accuracy: 0.9016 - val_loss: 0.2489 - val_accuracy: 0.8125 Epoch 274/500 61/61 [==============================] - 0s 148us/step - loss: 0.2640 - accuracy: 0.8852 - val_loss: 0.2525 - val_accuracy: 0.8125 Epoch 275/500 61/61 [==============================] - 0s 82us/step - loss: 0.2500 - accuracy: 0.8689 - val_loss: 0.2587 - val_accuracy: 0.8125 Epoch 276/500 61/61 [==============================] - 0s 115us/step - loss: 0.2310 - accuracy: 0.9180 - val_loss: 0.2640 - val_accuracy: 0.8125 Epoch 277/500 61/61 [==============================] - 0s 82us/step - loss: 0.2529 - accuracy: 0.8852 - val_loss: 0.2674 - val_accuracy: 0.8125 Epoch 278/500 61/61 [==============================] - 0s 98us/step - loss: 0.2270 - accuracy: 0.9180 - val_loss: 0.2698 - val_accuracy: 0.8125 Epoch 279/500 61/61 [==============================] - 0s 213us/step - loss: 0.2694 - accuracy: 0.8361 - val_loss: 0.2689 - val_accuracy: 0.8125 Epoch 280/500 61/61 [==============================] - 0s 82us/step - loss: 0.2231 - accuracy: 0.9344 - val_loss: 0.2661 - val_accuracy: 0.8125 Epoch 281/500 61/61 [==============================] - 0s 66us/step - loss: 0.1823 - accuracy: 0.9016 - val_loss: 0.2613 - val_accuracy: 0.8125 Epoch 282/500 61/61 [==============================] - 0s 82us/step - loss: 0.2410 - accuracy: 0.9016 - val_loss: 0.2570 - val_accuracy: 0.8125 Epoch 283/500 61/61 [==============================] - 0s 148us/step - loss: 0.2505 - accuracy: 0.8852 - val_loss: 0.2521 - val_accuracy: 0.8125 Epoch 284/500 61/61 [==============================] - 0s 66us/step - loss: 0.2127 - accuracy: 0.9016 - val_loss: 0.2491 - val_accuracy: 0.8125 Epoch 285/500 61/61 [==============================] - 0s 82us/step - loss: 0.2553 - accuracy: 0.8525 - val_loss: 0.2479 - val_accuracy: 0.8125 Epoch 286/500 61/61 [==============================] - 0s 82us/step - loss: 0.1868 - accuracy: 0.9508 - val_loss: 0.2462 - val_accuracy: 0.8125 Epoch 287/500 61/61 [==============================] - 0s 98us/step - loss: 0.2307 - accuracy: 0.9344 - val_loss: 0.2455 - val_accuracy: 0.8125 Epoch 288/500 61/61 [==============================] - 0s 164us/step - loss: 0.2236 - accuracy: 0.9180 - val_loss: 0.2471 - val_accuracy: 0.8125 Epoch 289/500 61/61 [==============================] - 0s 98us/step - loss: 0.2348 - accuracy: 0.9180 - val_loss: 0.2465 - val_accuracy: 0.8125 Epoch 290/500 61/61 [==============================] - 0s 98us/step - loss: 0.2167 - accuracy: 0.9016 - val_loss: 0.2483 - val_accuracy: 0.8125 Epoch 291/500 61/61 [==============================] - 0s 639us/step - loss: 0.2219 - accuracy: 0.8852 - val_loss: 0.2497 - val_accuracy: 0.8125 Epoch 292/500 61/61 [==============================] - 0s 131us/step - loss: 0.2422 - accuracy: 0.8689 - val_loss: 0.2520 - val_accuracy: 0.8125 Epoch 293/500 61/61 [==============================] - 0s 82us/step - loss: 0.2297 - accuracy: 0.9016 - val_loss: 0.2536 - val_accuracy: 0.8125 Epoch 294/500 61/61 [==============================] - 0s 66us/step - loss: 0.2210 - accuracy: 0.8852 - val_loss: 0.2534 - val_accuracy: 0.8125 Epoch 295/500 61/61 [==============================] - 0s 115us/step - loss: 0.2794 - accuracy: 0.8689 - val_loss: 0.2511 - val_accuracy: 0.8125 Epoch 296/500 61/61 [==============================] - 0s 131us/step - loss: 0.1848 - accuracy: 0.9180 - val_loss: 0.2501 - val_accuracy: 0.8125 Epoch 297/500 61/61 [==============================] - 0s 82us/step - loss: 0.2673 - accuracy: 0.8852 - val_loss: 0.2493 - val_accuracy: 0.8125 Epoch 298/500 61/61 [==============================] - 0s 82us/step - loss: 0.2225 - accuracy: 0.9344 - val_loss: 0.2485 - val_accuracy: 0.8125 Epoch 299/500 61/61 [==============================] - 0s 82us/step - loss: 0.1865 - accuracy: 0.9344 - val_loss: 0.2465 - val_accuracy: 0.8125 Epoch 300/500 61/61 [==============================] - 0s 82us/step - loss: 0.2294 - accuracy: 0.8852 - val_loss: 0.2458 - val_accuracy: 0.8125 Epoch 301/500 61/61 [==============================] - 0s 98us/step - loss: 0.2424 - accuracy: 0.9016 - val_loss: 0.2463 - val_accuracy: 0.8125 Epoch 302/500 61/61 [==============================] - 0s 66us/step - loss: 0.2130 - accuracy: 0.9344 - val_loss: 0.2476 - val_accuracy: 0.8125 Epoch 303/500 61/61 [==============================] - 0s 82us/step - loss: 0.2065 - accuracy: 0.9672 - val_loss: 0.2484 - val_accuracy: 0.8125 Epoch 304/500 61/61 [==============================] - 0s 98us/step - loss: 0.2230 - accuracy: 0.9344 - val_loss: 0.2479 - val_accuracy: 0.8125 Epoch 305/500 61/61 [==============================] - 0s 82us/step - loss: 0.1992 - accuracy: 0.9016 - val_loss: 0.2469 - val_accuracy: 0.8125 Epoch 306/500 61/61 [==============================] - 0s 66us/step - loss: 0.2198 - accuracy: 0.9180 - val_loss: 0.2452 - val_accuracy: 0.8125 Epoch 307/500 61/61 [==============================] - 0s 82us/step - loss: 0.2542 - accuracy: 0.8689 - val_loss: 0.2490 - val_accuracy: 0.8125 Epoch 308/500 61/61 [==============================] - 0s 98us/step - loss: 0.2403 - accuracy: 0.9016 - val_loss: 0.2506 - val_accuracy: 0.8125 Epoch 309/500 61/61 [==============================] - 0s 98us/step - loss: 0.2178 - accuracy: 0.9016 - val_loss: 0.2534 - val_accuracy: 0.8125 Epoch 310/500 61/61 [==============================] - 0s 82us/step - loss: 0.2346 - accuracy: 0.9180 - val_loss: 0.2559 - val_accuracy: 0.8125 Epoch 311/500 61/61 [==============================] - 0s 262us/step - loss: 0.1837 - accuracy: 0.9344 - val_loss: 0.2564 - val_accuracy: 0.8125 Epoch 312/500 61/61 [==============================] - 0s 82us/step - loss: 0.2127 - accuracy: 0.9016 - val_loss: 0.2571 - val_accuracy: 0.8125 Epoch 313/500 61/61 [==============================] - 0s 98us/step - loss: 0.2043 - accuracy: 0.9016 - val_loss: 0.2578 - val_accuracy: 0.8125 Epoch 314/500 61/61 [==============================] - 0s 82us/step - loss: 0.2151 - accuracy: 0.9180 - val_loss: 0.2593 - val_accuracy: 0.8125 Epoch 315/500 61/61 [==============================] - 0s 82us/step - loss: 0.1986 - accuracy: 0.9180 - val_loss: 0.2606 - val_accuracy: 0.8125 Epoch 316/500 61/61 [==============================] - 0s 82us/step - loss: 0.1828 - accuracy: 0.9508 - val_loss: 0.2595 - val_accuracy: 0.8125 Epoch 317/500 61/61 [==============================] - 0s 98us/step - loss: 0.1994 - accuracy: 0.9672 - val_loss: 0.2595 - val_accuracy: 0.8125 Epoch 318/500 61/61 [==============================] - 0s 82us/step - loss: 0.2413 - accuracy: 0.8852 - val_loss: 0.2608 - val_accuracy: 0.8125 Epoch 319/500 61/61 [==============================] - 0s 98us/step - loss: 0.2271 - accuracy: 0.9180 - val_loss: 0.2600 - val_accuracy: 0.8125 Epoch 320/500 61/61 [==============================] - 0s 66us/step - loss: 0.1769 - accuracy: 0.9180 - val_loss: 0.2572 - val_accuracy: 0.8125 Epoch 321/500 61/61 [==============================] - 0s 82us/step - loss: 0.1797 - accuracy: 0.9180 - val_loss: 0.2540 - val_accuracy: 0.8125 Epoch 322/500 61/61 [==============================] - 0s 98us/step - loss: 0.3119 - accuracy: 0.8525 - val_loss: 0.2519 - val_accuracy: 0.8125 Epoch 323/500 61/61 [==============================] - 0s 66us/step - loss: 0.1934 - accuracy: 0.9344 - val_loss: 0.2497 - val_accuracy: 0.8125 Epoch 324/500 61/61 [==============================] - 0s 66us/step - loss: 0.1894 - accuracy: 0.9180 - val_loss: 0.2487 - val_accuracy: 0.8125 Epoch 325/500 61/61 [==============================] - 0s 82us/step - loss: 0.1932 - accuracy: 0.9016 - val_loss: 0.2487 - val_accuracy: 0.8125 Epoch 326/500 61/61 [==============================] - 0s 98us/step - loss: 0.1893 - accuracy: 0.9016 - val_loss: 0.2491 - val_accuracy: 0.8125 Epoch 327/500 61/61 [==============================] - 0s 82us/step - loss: 0.2358 - accuracy: 0.9344 - val_loss: 0.2483 - val_accuracy: 0.8125 Epoch 328/500 61/61 [==============================] - 0s 82us/step - loss: 0.2373 - accuracy: 0.9016 - val_loss: 0.2514 - val_accuracy: 0.8125 Epoch 329/500 61/61 [==============================] - 0s 98us/step - loss: 0.2044 - accuracy: 0.9180 - val_loss: 0.2548 - val_accuracy: 0.8125 Epoch 330/500 61/61 [==============================] - 0s 82us/step - loss: 0.2210 - accuracy: 0.9180 - val_loss: 0.2584 - val_accuracy: 0.8125 Epoch 331/500 61/61 [==============================] - 0s 98us/step - loss: 0.2120 - accuracy: 0.9180 - val_loss: 0.2625 - val_accuracy: 0.8125 Epoch 332/500 61/61 [==============================] - 0s 98us/step - loss: 0.1945 - accuracy: 0.9344 - val_loss: 0.2650 - val_accuracy: 0.7500 Epoch 333/500 61/61 [==============================] - 0s 82us/step - loss: 0.2355 - accuracy: 0.9016 - val_loss: 0.2646 - val_accuracy: 0.7500 Epoch 334/500 61/61 [==============================] - 0s 82us/step - loss: 0.2288 - accuracy: 0.9016 - val_loss: 0.2638 - val_accuracy: 0.7500 Epoch 335/500 61/61 [==============================] - 0s 98us/step - loss: 0.2538 - accuracy: 0.8689 - val_loss: 0.2627 - val_accuracy: 0.7500 Epoch 336/500 61/61 [==============================] - 0s 98us/step - loss: 0.2297 - accuracy: 0.9180 - val_loss: 0.2600 - val_accuracy: 0.7500 Epoch 337/500 61/61 [==============================] - 0s 82us/step - loss: 0.1905 - accuracy: 0.9016 - val_loss: 0.2551 - val_accuracy: 0.8125 Epoch 338/500 61/61 [==============================] - 0s 98us/step - loss: 0.2163 - accuracy: 0.9016 - val_loss: 0.2545 - val_accuracy: 0.8125 Epoch 339/500 61/61 [==============================] - 0s 82us/step - loss: 0.2098 - accuracy: 0.9016 - val_loss: 0.2546 - val_accuracy: 0.8125 Epoch 340/500 61/61 [==============================] - 0s 82us/step - loss: 0.1861 - accuracy: 0.9180 - val_loss: 0.2530 - val_accuracy: 0.8125 Epoch 341/500 61/61 [==============================] - 0s 66us/step - loss: 0.2605 - accuracy: 0.8689 - val_loss: 0.2570 - val_accuracy: 0.7500 Epoch 342/500 61/61 [==============================] - 0s 82us/step - loss: 0.2137 - accuracy: 0.9508 - val_loss: 0.2576 - val_accuracy: 0.7500 Epoch 343/500 61/61 [==============================] - 0s 98us/step - loss: 0.2051 - accuracy: 0.9180 - val_loss: 0.2582 - val_accuracy: 0.7500 Epoch 344/500 61/61 [==============================] - 0s 82us/step - loss: 0.2342 - accuracy: 0.8852 - val_loss: 0.2583 - val_accuracy: 0.7500 Epoch 345/500 61/61 [==============================] - 0s 148us/step - loss: 0.2305 - accuracy: 0.9180 - val_loss: 0.2566 - val_accuracy: 0.7500 Epoch 346/500 61/61 [==============================] - 0s 82us/step - loss: 0.2201 - accuracy: 0.9180 - val_loss: 0.2541 - val_accuracy: 0.7500 Epoch 347/500 61/61 [==============================] - 0s 98us/step - loss: 0.1671 - accuracy: 0.9344 - val_loss: 0.2504 - val_accuracy: 0.8125 Epoch 348/500 61/61 [==============================] - 0s 82us/step - loss: 0.2590 - accuracy: 0.8525 - val_loss: 0.2501 - val_accuracy: 0.8125 Epoch 349/500 61/61 [==============================] - 0s 82us/step - loss: 0.1801 - accuracy: 0.9180 - val_loss: 0.2486 - val_accuracy: 0.8125 Epoch 350/500 61/61 [==============================] - 0s 82us/step - loss: 0.2126 - accuracy: 0.9344 - val_loss: 0.2505 - val_accuracy: 0.8125 Epoch 351/500 61/61 [==============================] - 0s 82us/step - loss: 0.2077 - accuracy: 0.9344 - val_loss: 0.2518 - val_accuracy: 0.8125 Epoch 352/500 61/61 [==============================] - 0s 82us/step - loss: 0.2365 - accuracy: 0.8689 - val_loss: 0.2529 - val_accuracy: 0.8125 Epoch 353/500 61/61 [==============================] - 0s 82us/step - loss: 0.2442 - accuracy: 0.8361 - val_loss: 0.2547 - val_accuracy: 0.8125 Epoch 354/500 61/61 [==============================] - 0s 98us/step - loss: 0.1952 - accuracy: 0.9344 - val_loss: 0.2559 - val_accuracy: 0.8125 Epoch 355/500 61/61 [==============================] - 0s 66us/step - loss: 0.1993 - accuracy: 0.9344 - val_loss: 0.2557 - val_accuracy: 0.8125 Epoch 356/500 61/61 [==============================] - 0s 66us/step - loss: 0.2259 - accuracy: 0.9016 - val_loss: 0.2521 - val_accuracy: 0.8125 Epoch 357/500 61/61 [==============================] - 0s 82us/step - loss: 0.1801 - accuracy: 0.9344 - val_loss: 0.2465 - val_accuracy: 0.8125 Epoch 358/500 61/61 [==============================] - 0s 82us/step - loss: 0.2513 - accuracy: 0.9016 - val_loss: 0.2411 - val_accuracy: 0.8750 Epoch 359/500 61/61 [==============================] - 0s 82us/step - loss: 0.2122 - accuracy: 0.8689 - val_loss: 0.2355 - val_accuracy: 0.9375 Epoch 360/500 61/61 [==============================] - 0s 98us/step - loss: 0.2011 - accuracy: 0.9016 - val_loss: 0.2313 - val_accuracy: 0.9375 Epoch 361/500 61/61 [==============================] - 0s 98us/step - loss: 0.2467 - accuracy: 0.9180 - val_loss: 0.2286 - val_accuracy: 0.9375 Epoch 362/500 61/61 [==============================] - 0s 82us/step - loss: 0.1961 - accuracy: 0.9344 - val_loss: 0.2267 - val_accuracy: 0.9375 Epoch 363/500 61/61 [==============================] - 0s 98us/step - loss: 0.2149 - accuracy: 0.9016 - val_loss: 0.2277 - val_accuracy: 0.9375 Epoch 364/500 61/61 [==============================] - 0s 82us/step - loss: 0.2042 - accuracy: 0.9180 - val_loss: 0.2305 - val_accuracy: 0.9375 Epoch 365/500 61/61 [==============================] - 0s 82us/step - loss: 0.1786 - accuracy: 0.9180 - val_loss: 0.2350 - val_accuracy: 0.9375 Epoch 366/500 61/61 [==============================] - 0s 82us/step - loss: 0.2251 - accuracy: 0.9344 - val_loss: 0.2408 - val_accuracy: 0.9375 Epoch 367/500 61/61 [==============================] - 0s 98us/step - loss: 0.1799 - accuracy: 0.9180 - val_loss: 0.2445 - val_accuracy: 0.8125 Epoch 368/500 61/61 [==============================] - 0s 98us/step - loss: 0.1615 - accuracy: 0.9180 - val_loss: 0.2451 - val_accuracy: 0.8125 Epoch 369/500 61/61 [==============================] - 0s 98us/step - loss: 0.2220 - accuracy: 0.9180 - val_loss: 0.2463 - val_accuracy: 0.8125 Epoch 370/500 61/61 [==============================] - 0s 82us/step - loss: 0.1721 - accuracy: 0.9344 - val_loss: 0.2446 - val_accuracy: 0.8750 Epoch 371/500 61/61 [==============================] - 0s 82us/step - loss: 0.1610 - accuracy: 0.9508 - val_loss: 0.2407 - val_accuracy: 0.9375 Epoch 372/500 61/61 [==============================] - 0s 82us/step - loss: 0.1506 - accuracy: 0.9508 - val_loss: 0.2377 - val_accuracy: 0.9375 Epoch 373/500 61/61 [==============================] - 0s 82us/step - loss: 0.2031 - accuracy: 0.9016 - val_loss: 0.2381 - val_accuracy: 0.9375 Epoch 374/500 61/61 [==============================] - 0s 82us/step - loss: 0.2078 - accuracy: 0.9180 - val_loss: 0.2387 - val_accuracy: 0.9375 Epoch 375/500 61/61 [==============================] - 0s 66us/step - loss: 0.1715 - accuracy: 0.9508 - val_loss: 0.2390 - val_accuracy: 0.9375 Epoch 376/500 61/61 [==============================] - 0s 98us/step - loss: 0.2494 - accuracy: 0.8689 - val_loss: 0.2385 - val_accuracy: 0.9375 Epoch 377/500 61/61 [==============================] - 0s 66us/step - loss: 0.2166 - accuracy: 0.9180 - val_loss: 0.2369 - val_accuracy: 0.9375 Epoch 378/500 61/61 [==============================] - 0s 82us/step - loss: 0.2292 - accuracy: 0.9016 - val_loss: 0.2330 - val_accuracy: 0.9375 Epoch 379/500 61/61 [==============================] - 0s 82us/step - loss: 0.2259 - accuracy: 0.9016 - val_loss: 0.2349 - val_accuracy: 0.9375 Epoch 380/500 61/61 [==============================] - 0s 82us/step - loss: 0.2024 - accuracy: 0.8852 - val_loss: 0.2361 - val_accuracy: 0.9375 Epoch 381/500 61/61 [==============================] - 0s 82us/step - loss: 0.2032 - accuracy: 0.9016 - val_loss: 0.2382 - val_accuracy: 0.9375 Epoch 382/500 61/61 [==============================] - 0s 98us/step - loss: 0.2696 - accuracy: 0.8689 - val_loss: 0.2387 - val_accuracy: 0.9375 Epoch 383/500 61/61 [==============================] - 0s 98us/step - loss: 0.2123 - accuracy: 0.8689 - val_loss: 0.2416 - val_accuracy: 0.8750 Epoch 384/500 61/61 [==============================] - 0s 82us/step - loss: 0.1916 - accuracy: 0.9180 - val_loss: 0.2475 - val_accuracy: 0.8125 Epoch 385/500 61/61 [==============================] - 0s 82us/step - loss: 0.1934 - accuracy: 0.8852 - val_loss: 0.2530 - val_accuracy: 0.7500 Epoch 386/500 61/61 [==============================] - 0s 98us/step - loss: 0.1802 - accuracy: 0.9344 - val_loss: 0.2579 - val_accuracy: 0.7500 Epoch 387/500 61/61 [==============================] - 0s 82us/step - loss: 0.1949 - accuracy: 0.8852 - val_loss: 0.2603 - val_accuracy: 0.7500 Epoch 388/500 61/61 [==============================] - 0s 98us/step - loss: 0.2090 - accuracy: 0.9508 - val_loss: 0.2633 - val_accuracy: 0.7500 Epoch 389/500 61/61 [==============================] - 0s 98us/step - loss: 0.2219 - accuracy: 0.9016 - val_loss: 0.2626 - val_accuracy: 0.7500 Epoch 390/500 61/61 [==============================] - 0s 98us/step - loss: 0.2252 - accuracy: 0.8852 - val_loss: 0.2580 - val_accuracy: 0.7500 Epoch 391/500 61/61 [==============================] - 0s 66us/step - loss: 0.1669 - accuracy: 0.9180 - val_loss: 0.2539 - val_accuracy: 0.7500 Epoch 392/500 61/61 [==============================] - 0s 82us/step - loss: 0.1752 - accuracy: 0.9344 - val_loss: 0.2471 - val_accuracy: 0.7500 Epoch 393/500 61/61 [==============================] - 0s 98us/step - loss: 0.1831 - accuracy: 0.9508 - val_loss: 0.2386 - val_accuracy: 0.9375 Epoch 394/500 61/61 [==============================] - 0s 82us/step - loss: 0.1932 - accuracy: 0.9508 - val_loss: 0.2302 - val_accuracy: 0.9375 Epoch 395/500 61/61 [==============================] - 0s 82us/step - loss: 0.1883 - accuracy: 0.9344 - val_loss: 0.2237 - val_accuracy: 0.9375 Epoch 396/500 61/61 [==============================] - 0s 82us/step - loss: 0.1856 - accuracy: 0.9508 - val_loss: 0.2202 - val_accuracy: 0.9375 Epoch 397/500 61/61 [==============================] - 0s 82us/step - loss: 0.1901 - accuracy: 0.9344 - val_loss: 0.2184 - val_accuracy: 0.9375 Epoch 398/500 61/61 [==============================] - 0s 82us/step - loss: 0.2337 - accuracy: 0.9016 - val_loss: 0.2216 - val_accuracy: 0.9375 Epoch 399/500 61/61 [==============================] - 0s 82us/step - loss: 0.1799 - accuracy: 0.9508 - val_loss: 0.2298 - val_accuracy: 0.9375 Epoch 400/500 61/61 [==============================] - 0s 82us/step - loss: 0.1853 - accuracy: 0.9016 - val_loss: 0.2390 - val_accuracy: 0.8125 Epoch 401/500 61/61 [==============================] - 0s 82us/step - loss: 0.2058 - accuracy: 0.9180 - val_loss: 0.2495 - val_accuracy: 0.8125 Epoch 402/500 61/61 [==============================] - 0s 98us/step - loss: 0.2064 - accuracy: 0.9180 - val_loss: 0.2590 - val_accuracy: 0.8125 Epoch 403/500 61/61 [==============================] - 0s 82us/step - loss: 0.1821 - accuracy: 0.8689 - val_loss: 0.2657 - val_accuracy: 0.7500 Epoch 404/500 61/61 [==============================] - 0s 82us/step - loss: 0.1386 - accuracy: 0.9344 - val_loss: 0.2709 - val_accuracy: 0.7500 Epoch 405/500 61/61 [==============================] - 0s 66us/step - loss: 0.1755 - accuracy: 0.9016 - val_loss: 0.2743 - val_accuracy: 0.7500 Epoch 406/500 61/61 [==============================] - 0s 98us/step - loss: 0.2198 - accuracy: 0.9016 - val_loss: 0.2742 - val_accuracy: 0.7500 Epoch 407/500 61/61 [==============================] - 0s 98us/step - loss: 0.1686 - accuracy: 0.9344 - val_loss: 0.2685 - val_accuracy: 0.7500 Epoch 408/500 61/61 [==============================] - 0s 82us/step - loss: 0.1962 - accuracy: 0.9180 - val_loss: 0.2587 - val_accuracy: 0.8125 Epoch 409/500 61/61 [==============================] - 0s 82us/step - loss: 0.2439 - accuracy: 0.8525 - val_loss: 0.2530 - val_accuracy: 0.8125 Epoch 410/500 61/61 [==============================] - 0s 66us/step - loss: 0.2253 - accuracy: 0.8852 - val_loss: 0.2461 - val_accuracy: 0.8125 Epoch 411/500 61/61 [==============================] - 0s 98us/step - loss: 0.1926 - accuracy: 0.9016 - val_loss: 0.2383 - val_accuracy: 0.8750 Epoch 412/500 61/61 [==============================] - 0s 82us/step - loss: 0.2157 - accuracy: 0.9016 - val_loss: 0.2353 - val_accuracy: 0.8750 Epoch 413/500 61/61 [==============================] - 0s 82us/step - loss: 0.1819 - accuracy: 0.9344 - val_loss: 0.2344 - val_accuracy: 0.8750 Epoch 414/500 61/61 [==============================] - 0s 82us/step - loss: 0.1828 - accuracy: 0.9508 - val_loss: 0.2347 - val_accuracy: 0.8750 Epoch 415/500 61/61 [==============================] - 0s 82us/step - loss: 0.1907 - accuracy: 0.9180 - val_loss: 0.2366 - val_accuracy: 0.8750 Epoch 416/500 61/61 [==============================] - 0s 98us/step - loss: 0.2130 - accuracy: 0.9180 - val_loss: 0.2411 - val_accuracy: 0.8750 Epoch 417/500 61/61 [==============================] - 0s 82us/step - loss: 0.2194 - accuracy: 0.8852 - val_loss: 0.2485 - val_accuracy: 0.8125 Epoch 418/500 61/61 [==============================] - 0s 82us/step - loss: 0.1797 - accuracy: 0.9344 - val_loss: 0.2542 - val_accuracy: 0.8125 Epoch 419/500 61/61 [==============================] - 0s 82us/step - loss: 0.1696 - accuracy: 0.9508 - val_loss: 0.2594 - val_accuracy: 0.7500 Epoch 420/500 61/61 [==============================] - 0s 98us/step - loss: 0.1598 - accuracy: 0.9344 - val_loss: 0.2622 - val_accuracy: 0.7500 Epoch 421/500 61/61 [==============================] - 0s 82us/step - loss: 0.2258 - accuracy: 0.9016 - val_loss: 0.2607 - val_accuracy: 0.7500 Epoch 422/500 61/61 [==============================] - 0s 98us/step - loss: 0.2256 - accuracy: 0.9016 - val_loss: 0.2554 - val_accuracy: 0.8125 Epoch 423/500 61/61 [==============================] - 0s 82us/step - loss: 0.1635 - accuracy: 0.9180 - val_loss: 0.2486 - val_accuracy: 0.8125 Epoch 424/500 61/61 [==============================] - 0s 66us/step - loss: 0.2081 - accuracy: 0.8852 - val_loss: 0.2430 - val_accuracy: 0.8125 Epoch 425/500 61/61 [==============================] - 0s 66us/step - loss: 0.2089 - accuracy: 0.9180 - val_loss: 0.2398 - val_accuracy: 0.8750 Epoch 426/500 61/61 [==============================] - 0s 82us/step - loss: 0.1807 - accuracy: 0.8852 - val_loss: 0.2408 - val_accuracy: 0.8750 Epoch 427/500 61/61 [==============================] - 0s 82us/step - loss: 0.1902 - accuracy: 0.9016 - val_loss: 0.2426 - val_accuracy: 0.8750 Epoch 428/500 61/61 [==============================] - 0s 82us/step - loss: 0.2096 - accuracy: 0.9344 - val_loss: 0.2486 - val_accuracy: 0.8125 Epoch 429/500 61/61 [==============================] - 0s 82us/step - loss: 0.1709 - accuracy: 0.9344 - val_loss: 0.2530 - val_accuracy: 0.8125 Epoch 430/500 61/61 [==============================] - 0s 82us/step - loss: 0.1769 - accuracy: 0.9508 - val_loss: 0.2598 - val_accuracy: 0.7500 Epoch 431/500 61/61 [==============================] - 0s 66us/step - loss: 0.2112 - accuracy: 0.9016 - val_loss: 0.2650 - val_accuracy: 0.7500 Epoch 432/500 61/61 [==============================] - 0s 115us/step - loss: 0.1478 - accuracy: 0.9344 - val_loss: 0.2713 - val_accuracy: 0.7500 Epoch 433/500 61/61 [==============================] - 0s 98us/step - loss: 0.2175 - accuracy: 0.8689 - val_loss: 0.2730 - val_accuracy: 0.7500 Epoch 434/500 61/61 [==============================] - 0s 98us/step - loss: 0.1325 - accuracy: 0.9672 - val_loss: 0.2737 - val_accuracy: 0.7500 Epoch 435/500 61/61 [==============================] - 0s 82us/step - loss: 0.1561 - accuracy: 0.9508 - val_loss: 0.2738 - val_accuracy: 0.7500 Epoch 436/500 61/61 [==============================] - 0s 82us/step - loss: 0.2017 - accuracy: 0.8852 - val_loss: 0.2704 - val_accuracy: 0.7500 Epoch 437/500 61/61 [==============================] - 0s 131us/step - loss: 0.1876 - accuracy: 0.9180 - val_loss: 0.2654 - val_accuracy: 0.7500 Epoch 438/500 61/61 [==============================] - 0s 98us/step - loss: 0.2258 - accuracy: 0.8689 - val_loss: 0.2600 - val_accuracy: 0.7500 Epoch 439/500 61/61 [==============================] - 0s 82us/step - loss: 0.1539 - accuracy: 0.9344 - val_loss: 0.2531 - val_accuracy: 0.8125 Epoch 440/500 61/61 [==============================] - 0s 82us/step - loss: 0.2110 - accuracy: 0.8852 - val_loss: 0.2459 - val_accuracy: 0.8750 Epoch 441/500 61/61 [==============================] - 0s 82us/step - loss: 0.1914 - accuracy: 0.9508 - val_loss: 0.2425 - val_accuracy: 0.8750 Epoch 442/500 61/61 [==============================] - 0s 98us/step - loss: 0.1547 - accuracy: 0.9672 - val_loss: 0.2424 - val_accuracy: 0.8750 Epoch 443/500 61/61 [==============================] - 0s 98us/step - loss: 0.2022 - accuracy: 0.9672 - val_loss: 0.2412 - val_accuracy: 0.8750 Epoch 444/500 61/61 [==============================] - 0s 82us/step - loss: 0.1613 - accuracy: 0.9508 - val_loss: 0.2409 - val_accuracy: 0.8750 Epoch 445/500 61/61 [==============================] - 0s 82us/step - loss: 0.1712 - accuracy: 0.9180 - val_loss: 0.2415 - val_accuracy: 0.8750 Epoch 446/500 61/61 [==============================] - 0s 98us/step - loss: 0.2337 - accuracy: 0.9180 - val_loss: 0.2431 - val_accuracy: 0.8750 Epoch 447/500 61/61 [==============================] - 0s 82us/step - loss: 0.1182 - accuracy: 0.9836 - val_loss: 0.2446 - val_accuracy: 0.8750 Epoch 448/500 61/61 [==============================] - 0s 82us/step - loss: 0.1672 - accuracy: 0.9508 - val_loss: 0.2457 - val_accuracy: 0.8750 Epoch 449/500 61/61 [==============================] - 0s 98us/step - loss: 0.2051 - accuracy: 0.9180 - val_loss: 0.2475 - val_accuracy: 0.8750 Epoch 450/500 61/61 [==============================] - 0s 82us/step - loss: 0.1295 - accuracy: 0.9508 - val_loss: 0.2497 - val_accuracy: 0.8750 Epoch 451/500 61/61 [==============================] - 0s 82us/step - loss: 0.1465 - accuracy: 0.9508 - val_loss: 0.2529 - val_accuracy: 0.8750 Epoch 452/500 61/61 [==============================] - 0s 82us/step - loss: 0.1662 - accuracy: 0.9344 - val_loss: 0.2563 - val_accuracy: 0.8750 Epoch 453/500 61/61 [==============================] - 0s 115us/step - loss: 0.1826 - accuracy: 0.9344 - val_loss: 0.2611 - val_accuracy: 0.7500 Epoch 454/500 61/61 [==============================] - 0s 82us/step - loss: 0.2557 - accuracy: 0.8852 - val_loss: 0.2714 - val_accuracy: 0.7500 Epoch 455/500 61/61 [==============================] - 0s 82us/step - loss: 0.1743 - accuracy: 0.9180 - val_loss: 0.2782 - val_accuracy: 0.7500 Epoch 456/500 61/61 [==============================] - 0s 131us/step - loss: 0.2361 - accuracy: 0.8525 - val_loss: 0.2795 - val_accuracy: 0.7500 Epoch 457/500 61/61 [==============================] - 0s 98us/step - loss: 0.1827 - accuracy: 0.9016 - val_loss: 0.2764 - val_accuracy: 0.7500 Epoch 458/500 61/61 [==============================] - 0s 115us/step - loss: 0.1765 - accuracy: 0.9344 - val_loss: 0.2686 - val_accuracy: 0.8125 Epoch 459/500 61/61 [==============================] - 0s 82us/step - loss: 0.2066 - accuracy: 0.9016 - val_loss: 0.2597 - val_accuracy: 0.8750 Epoch 460/500 61/61 [==============================] - 0s 82us/step - loss: 0.1442 - accuracy: 0.9508 - val_loss: 0.2490 - val_accuracy: 0.8750 Epoch 461/500 61/61 [==============================] - 0s 82us/step - loss: 0.1476 - accuracy: 0.9344 - val_loss: 0.2383 - val_accuracy: 0.8750 Epoch 462/500 61/61 [==============================] - 0s 82us/step - loss: 0.1409 - accuracy: 0.9180 - val_loss: 0.2298 - val_accuracy: 0.8750 Epoch 463/500 61/61 [==============================] - 0s 98us/step - loss: 0.2610 - accuracy: 0.9016 - val_loss: 0.2349 - val_accuracy: 0.8750 Epoch 464/500 61/61 [==============================] - 0s 82us/step - loss: 0.1266 - accuracy: 0.9672 - val_loss: 0.2404 - val_accuracy: 0.8750 Epoch 465/500 61/61 [==============================] - 0s 82us/step - loss: 0.1839 - accuracy: 0.9016 - val_loss: 0.2472 - val_accuracy: 0.8750 Epoch 466/500 61/61 [==============================] - 0s 98us/step - loss: 0.1637 - accuracy: 0.9344 - val_loss: 0.2530 - val_accuracy: 0.8750 Epoch 467/500 61/61 [==============================] - 0s 82us/step - loss: 0.1247 - accuracy: 0.9672 - val_loss: 0.2576 - val_accuracy: 0.8750 Epoch 468/500 61/61 [==============================] - 0s 82us/step - loss: 0.1433 - accuracy: 0.9672 - val_loss: 0.2594 - val_accuracy: 0.8750 Epoch 469/500 61/61 [==============================] - 0s 82us/step - loss: 0.1737 - accuracy: 0.9344 - val_loss: 0.2568 - val_accuracy: 0.8750 Epoch 470/500 61/61 [==============================] - 0s 82us/step - loss: 0.1840 - accuracy: 0.9508 - val_loss: 0.2539 - val_accuracy: 0.8750 Epoch 471/500 61/61 [==============================] - 0s 98us/step - loss: 0.1607 - accuracy: 0.9508 - val_loss: 0.2500 - val_accuracy: 0.8750 Epoch 472/500 61/61 [==============================] - 0s 82us/step - loss: 0.1648 - accuracy: 0.9344 - val_loss: 0.2471 - val_accuracy: 0.8750 Epoch 473/500 61/61 [==============================] - 0s 66us/step - loss: 0.1761 - accuracy: 0.8852 - val_loss: 0.2476 - val_accuracy: 0.8750 Epoch 474/500 61/61 [==============================] - 0s 66us/step - loss: 0.2031 - accuracy: 0.8852 - val_loss: 0.2470 - val_accuracy: 0.8750 Epoch 475/500 61/61 [==============================] - 0s 82us/step - loss: 0.1676 - accuracy: 0.9180 - val_loss: 0.2477 - val_accuracy: 0.8750 Epoch 476/500 61/61 [==============================] - 0s 82us/step - loss: 0.1651 - accuracy: 0.9180 - val_loss: 0.2491 - val_accuracy: 0.8750 Epoch 477/500 61/61 [==============================] - 0s 98us/step - loss: 0.1679 - accuracy: 0.9180 - val_loss: 0.2509 - val_accuracy: 0.8750 Epoch 478/500 61/61 [==============================] - 0s 82us/step - loss: 0.1580 - accuracy: 0.9344 - val_loss: 0.2491 - val_accuracy: 0.8750 Epoch 479/500 61/61 [==============================] - 0s 82us/step - loss: 0.1720 - accuracy: 0.9180 - val_loss: 0.2474 - val_accuracy: 0.8750 Epoch 480/500 61/61 [==============================] - 0s 82us/step - loss: 0.1907 - accuracy: 0.8852 - val_loss: 0.2461 - val_accuracy: 0.8750 Epoch 481/500 61/61 [==============================] - 0s 66us/step - loss: 0.1130 - accuracy: 0.9672 - val_loss: 0.2425 - val_accuracy: 0.8750 Epoch 482/500 61/61 [==============================] - 0s 98us/step - loss: 0.1458 - accuracy: 0.9508 - val_loss: 0.2412 - val_accuracy: 0.8750 Epoch 483/500 61/61 [==============================] - 0s 82us/step - loss: 0.1653 - accuracy: 0.9344 - val_loss: 0.2394 - val_accuracy: 0.8750 Epoch 484/500 61/61 [==============================] - 0s 66us/step - loss: 0.1788 - accuracy: 0.9508 - val_loss: 0.2408 - val_accuracy: 0.8750 Epoch 485/500 61/61 [==============================] - 0s 98us/step - loss: 0.1681 - accuracy: 0.9344 - val_loss: 0.2447 - val_accuracy: 0.8750 Epoch 486/500 61/61 [==============================] - 0s 98us/step - loss: 0.1824 - accuracy: 0.9344 - val_loss: 0.2498 - val_accuracy: 0.8750 Epoch 487/500 61/61 [==============================] - 0s 98us/step - loss: 0.1492 - accuracy: 0.9508 - val_loss: 0.2612 - val_accuracy: 0.8125 Epoch 488/500 61/61 [==============================] - 0s 82us/step - loss: 0.1702 - accuracy: 0.9016 - val_loss: 0.2706 - val_accuracy: 0.8125 Epoch 489/500 61/61 [==============================] - 0s 115us/step - loss: 0.1891 - accuracy: 0.9016 - val_loss: 0.2792 - val_accuracy: 0.7500 Epoch 490/500 61/61 [==============================] - 0s 82us/step - loss: 0.1797 - accuracy: 0.9016 - val_loss: 0.2878 - val_accuracy: 0.7500 Epoch 491/500 61/61 [==============================] - 0s 82us/step - loss: 0.1879 - accuracy: 0.8852 - val_loss: 0.2902 - val_accuracy: 0.7500 Epoch 492/500 61/61 [==============================] - 0s 82us/step - loss: 0.1742 - accuracy: 0.9180 - val_loss: 0.2883 - val_accuracy: 0.7500 Epoch 493/500 61/61 [==============================] - 0s 82us/step - loss: 0.1652 - accuracy: 0.9016 - val_loss: 0.2824 - val_accuracy: 0.7500 Epoch 494/500 61/61 [==============================] - 0s 82us/step - loss: 0.1693 - accuracy: 0.9016 - val_loss: 0.2735 - val_accuracy: 0.8125 Epoch 495/500 61/61 [==============================] - 0s 82us/step - loss: 0.1291 - accuracy: 0.9344 - val_loss: 0.2636 - val_accuracy: 0.8125 Epoch 496/500 61/61 [==============================] - 0s 115us/step - loss: 0.1865 - accuracy: 0.9016 - val_loss: 0.2517 - val_accuracy: 0.8750 Epoch 497/500 61/61 [==============================] - 0s 98us/step - loss: 0.1175 - accuracy: 0.9508 - val_loss: 0.2411 - val_accuracy: 0.8750 Epoch 498/500 61/61 [==============================] - 0s 66us/step - loss: 0.1466 - accuracy: 0.9672 - val_loss: 0.2368 - val_accuracy: 0.8750 Epoch 499/500 61/61 [==============================] - 0s 98us/step - loss: 0.1784 - accuracy: 0.9344 - val_loss: 0.2378 - val_accuracy: 0.8750 Epoch 500/500 61/61 [==============================] - 0s 98us/step - loss: 0.1750 - accuracy: 0.9344 - val_loss: 0.2393 - val_accuracy: 0.8750 . print(history.history.keys()) . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Model Loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;],loc=&#39;upper left&#39;) plt.show() . nn_pred = nn.predict(X_test) nn_pred[nn_pred &gt; 0.5] = 1 nn_pred[nn_pred &lt;= 0.5] = 0 . print(nn_pred) . [[0.] [1.] [1.] [0.] [1.] [0.] [0.] [1.] [0.]] . eval_results = nn.evaluate(X_test, y_test, verbose=0) print(&quot; nLoss, accuracy on test data: &quot;) print(&quot;%0.4f %0.2f%%&quot; % (eval_results[0], eval_results[1]*100)) . Loss, accuracy on test data: 0.4953 77.78% . 6. Results . Cross Validation . print(&quot;Decision Tree&quot;) # print(&#39;Precision&#39;, np.mean(dt_precision)) # print(&#39;Recall&#39;, np.mean(dt_recall)) print(&#39;F1&#39;, np.mean(dt_f1)) print() print(&quot;Random Forest&quot;) # print(&#39;Precision&#39;, np.mean(rf_precision)) # print(&#39;Recall&#39;, np.mean(rf_recall)) print(&#39;F1&#39;, np.mean(rf_f1)) print() print(&quot;XGBoost&quot;) # print(&#39;Precision&#39;, np.mean(xgbc_precision)) # print(&#39;Recall&#39;, np.mean(xgbc_recall)) print(&#39;F1&#39;, np.mean(xgbc_f1)) . Decision Tree F1 0.6733333333333333 Random Forest F1 0.6647619047619048 XGBoost F1 0.7147619047619047 . Prediction . print(&quot;Decision Tree&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, dt_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, dt_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, dt_pred)) print() print(&quot;Random Forest&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, rf_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, rf_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, rf_pred)) print() print(&quot;XGBoost&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, xgbc_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, xgbc_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, xgbc_pred)) print() print(&quot;NN&quot;) print(&quot;Precision: &quot;, sklearn.metrics.accuracy_score(y_test, nn_pred)) print(&quot;Recal: &quot;, sklearn.metrics.recall_score(y_test, nn_pred)) print(&quot;F1: &quot;, sklearn.metrics.f1_score(y_test, nn_pred)) . Decision Tree Precision: 1.0 Recal: 1.0 F1: 1.0 Random Forest Precision: 0.7777777777777778 Recal: 0.5 F1: 0.6666666666666666 XGBoost Precision: 0.7777777777777778 Recal: 1.0 F1: 0.8 NN Precision: 0.7777777777777778 Recal: 0.75 F1: 0.75 . 7. Explain . 7.1 Partial Dependence Plot . from sklearn.inspection import plot_partial_dependence . X_train.columns . Index([&#39;No&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;NTproBNP&#39;, &#39;PCT1&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Phlegm&#39;, &#39;SoreThroat&#39;, &#39;Headache&#39;, &#39;Fatigue&#39;, &#39;Diarrhea&#39;, &#39;NauseaNVomit&#39;, &#39;CAD&#39;], dtype=&#39;object&#39;) . plot_partial_dependence(rf, X_train, [1]) . &lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x1625a6d8&gt; . plot_partial_dependence(dt, X_train, [3, 4, 5]) . &lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x15841d30&gt; . plot_partial_dependence(dt, X_train, [6, 7, 8]) . &lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x1cd78b38&gt; . def my_pdp(model, X, feat_idx): X = np.array(X) fmax, fmin = np.max(X[:, feat_idx]), np.min(X[:, feat_idx]) frange = np.linspace(fmin, fmax, 100) preds = [] for x in frange: X_ = X.copy() X_[:, feat_idx] = x pred = model.predict(X_) preds.append(np.mean(pred)) return (frange, np.array(preds)) . my_data = my_pdp(dt, X_train, 6) # sk_data = partial_dependence(dtr, X = X_train, features = [6], percentiles=[0,1]) plt.subplot(121) plt.plot(my_data[0], my_data[1]) # plt.subplot(122) # plt.plot(sk_data[1][0], sk_data[0][0]) plt.show() . 7.2 Individual Conditional Expectation (ICE) . def my_ice(model, X, feat_idx): # X = np.array(X) fmax, fmin = np.max(np.array(X)[:, feat_idx]), np.min(np.array(X)[:, feat_idx]) frange = np.linspace(fmin, fmax, 100) preds = [] for x in frange: X_ = X.copy() X_.iloc[:, feat_idx] = x pred = model.predict_proba(X_) # print(pred.shape) preds.append(pred[:, 1]) return (frange, np.array(preds)) . my_data = my_ice(xgbc, X_train, 1) # sk_data = partial_dependence(dtr, X = X_train, features = [6], percentiles=[0,1]) . f = plt.figure(figsize=(20, 5)) plt.subplot(121) plt.plot(my_data[0], my_data[1]) # plt.subplot(122) # plt.plot(sk_data[1][0], sk_data[0][0]) plt.show() . 7.3 Accumulated Local Effects (ALE) Plot . from alibi.explainers import ALE, plot_ale . nn_ale = ALE(rf.predict_proba, feature_names=X_train.columns, target_names=[&#39;normal&#39;]) . exp = nn_ale.explain(X_train.to_numpy()) . plot_ale(exp, features=[0], fig_kw={&#39;figwidth&#39;:5, &#39;figheight&#39;: 5}) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000059B2320&gt;]], dtype=object) . 7.4 Feature Interaction . H-Statistic . 7.5 Permutation Feature Importance . from sklearn.inspection import permutation_importance from matplotlib import pyplot . results = permutation_importance(rf, X_train, y_train, scoring=&#39;accuracy&#39;) . # get importance importance = results.importances_mean # summarize feature importance for i,v in enumerate(importance): print(&#39;Feature: %20s, t Score: %.3f t Actual: %.3f&#39; % (X_train.columns[i],v, rf.feature_importances_[i])) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() . Feature: No, Score: 0.016 Actual: 0.056 Feature: Age, Score: 0.013 Actual: 0.052 Feature: AgeG1, Score: 0.000 Actual: 0.010 Feature: cTnITimes, Score: 0.000 Actual: 0.093 Feature: cTnI, Score: 0.013 Actual: 0.078 Feature: cTnICKMBOrdinal1, Score: 0.000 Actual: 0.011 Feature: NTproBNP, Score: 0.060 Actual: 0.213 Feature: PCT1, Score: 0.000 Actual: 0.019 Feature: LYM1, Score: 0.005 Actual: 0.037 Feature: N2L1, Score: 0.000 Actual: 0.039 Feature: CRP1, Score: 0.000 Actual: 0.074 Feature: CRP2, Score: 0.044 Actual: 0.162 Feature: ALB2, Score: 0.034 Actual: 0.097 Feature: Phlegm, Score: 0.000 Actual: 0.008 Feature: SoreThroat, Score: 0.000 Actual: 0.009 Feature: Headache, Score: 0.000 Actual: 0.001 Feature: Fatigue, Score: 0.000 Actual: 0.032 Feature: Diarrhea, Score: 0.000 Actual: 0.008 Feature: NauseaNVomit, Score: 0.000 Actual: 0.000 Feature: CAD, Score: 0.000 Actual: 0.000 . pyplot.bar([x for x in range(len(rf.feature_importances_))], rf.feature_importances_) pyplot.show() . 7.6 Global Surrogate . R-squared measure . y_s = rf.predict_proba(X_test) y_o = nn.predict_proba(X_test) . 1 - ( np.sum(np.power(y_s - y_o, 2)) / np.sum(np.power(y_o - np.mean(y_o), 2)) ) . -3.5200869290559718 . 7.7 LIME . from lime import lime_tabular . X_train.columns . Index([&#39;No&#39;, &#39;Age&#39;, &#39;AgeG1&#39;, &#39;cTnITimes&#39;, &#39;cTnI&#39;, &#39;cTnICKMBOrdinal1&#39;, &#39;NTproBNP&#39;, &#39;PCT1&#39;, &#39;LYM1&#39;, &#39;N2L1&#39;, &#39;CRP1&#39;, &#39;CRP2&#39;, &#39;ALB2&#39;, &#39;Phlegm&#39;, &#39;SoreThroat&#39;, &#39;Headache&#39;, &#39;Fatigue&#39;, &#39;Diarrhea&#39;, &#39;NauseaNVomit&#39;, &#39;CAD&#39;], dtype=&#39;object&#39;) . categorical_features = [2, 13, 14, 15, 15, 16, 17, 18, 19] categorical_names = {} for c in categorical_features: categorical_names[c] = [&quot;False&quot;, &quot;True&quot;] . feature_names = X_train.columns class_names = [&#39;normal&#39;, &#39;severe&#39;] . idx = 0 print(&#39;Probability(normal) =&#39;, rf.predict_proba(np.array(X_test)[idx, :].reshape(1, -1))[0][0]) . Probability(normal) = 1.0 . def lime_explain(idx, X_train, X_test, predict_method, feature_names, class_names, num_features): print(&#39;Patient id: %d&#39; % idx) print(&#39;Probability(normal) =&#39;, predict_method(np.array(X_test)[idx, :].reshape(1, -1))[0][0]) print(&#39;True class: %s&#39; % class_names[y_test[idx]]) explainer = lime_tabular.LimeTabularExplainer(np.array(X_train), feature_names=feature_names, class_names=class_names, categorical_features=categorical_features, categorical_names = categorical_names, discretize_continuous=True) return explainer.explain_instance(np.array(X_test)[idx, :], predict_method, num_features=num_features) . print(&quot;Pred: &quot;, xgbc_pred) print(&quot;True: &quot;, y_test) . Pred: [0 1 1 0 1 1 1 1 0] True: [0 1 0 0 1 1 0 1 0] . idx = 0 exp = lime_explain(idx, X_train, X_test, xgbc.predict_proba, feature_names, class_names, 20) . Patient id: 0 Probability(normal) = 0.97769076 True class: normal . %matplotlib inline fig = exp.as_pyplot_figure() . # exp.save_to_file(&#39;./covid19.html&#39;) . exp.show_in_notebook(show_table=True, show_all=False) . . . # sc.inverse_transform(X_test[idx]) . exp.as_list() . More Train Cases . y_pred = xgbc.predict(X_train) y_predprob = xgbc.predict_proba(X_train) . for i, j in enumerate(y_predprob): if np.abs(j[0] - j[1]) &lt; 0.2 * 2: print(j) print(i, &#39;Close&#39;, y_train[i]) if y_pred[i] != y_train[i]: print(j) print(&quot;[&quot;, i, &quot;]&quot;, &quot;Predict:&quot;, y_pred[i], &quot;Truth:&quot;, y_train[i]) . exp = lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names, categorical_features=categorical_features, categorical_names = categorical_names, discretize_continuous=True) . idx = 27 exp = exp.explain_instance(X_train[idx, :], xgbc.predict_proba, num_features=10) exp.show_in_notebook() . for i, f in enumerate(feature_names): print(f, X_train[27][i]) . More test cases . y_predt = xgbc.predict(X_test) y_predprobt = xgbc.predict_proba(X_test) . for i, j in enumerate(y_predprobt): if np.abs(j[0] - j[1]) &lt; 0.2 * 2: print(j) print(i, &#39;Close&#39;, y_test[i]) if y_predt[i] != y_test[i]: print(j) print(&quot;[&quot;, i, &quot;]&quot;, &quot;Predict:&quot;, y_predt[i], &quot;Truth:&quot;, y_test[i]) . exp = lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names, categorical_features=categorical_features, categorical_names = categorical_names, discretize_continuous=True) . idx = 2 exp = exp.explain_instance(X_test[idx, :], xgbc.predict_proba, num_features=10) exp.show_in_notebook() . y_predprobt . y_test . for i, f in enumerate(feature_names): print(f, X_test[2][i]) . 7.8 Scoped Rules (Anchors) . from alibi.explainers import AnchorTabular from alibi.utils.data import gen_category_map . explainer = AnchorTabular(rf.predict, X_train.columns) . explainer.fit(X_train.to_numpy(), disc_perc=[25, 50, 75]) . AnchorTabular(meta={ &#39;name&#39;: &#39;AnchorTabular&#39;, &#39;type&#39;: [&#39;blackbox&#39;], &#39;explanations&#39;: [&#39;local&#39;], &#39;params&#39;: {&#39;seed&#39;: None, &#39;disc_perc&#39;: [25, 50, 75]} }) . idx = 1 print(&#39;Prediction: &#39;, explainer.predictor(X_test.to_numpy()[idx].reshape(1, -1))[0]) print(y_test[idx]) . Prediction: 1 1 . explanation = explainer.explain(X_test.to_numpy()[idx].reshape(1, -1), threshold=0.8) print(&#39;Anchor: %s&#39; % (&#39; AND &#39;.join(explanation.anchor))) print(&#39;Precision: %.2f&#39; % explanation.precision) print(&#39;Coverage: %.2f&#39; % explanation.coverage) . Anchor: AgeG1 &lt;= 1.00 AND CRP2 &gt; 33.25 AND cTnITimes &gt; 4.00 Precision: 0.93 Coverage: 1.00 . 7.9 Shapley Values . Game Theory . 7.10 SHAP (SHapley Additive exPlanations) . Kernel Shap . import shap shap.initjs() . from alibi.explainers import KernelShap . explainer = KernelShap(xgbc.predict_proba) . explainer.fit(X_train) . KernelShap(meta={ &#39;name&#39;: &#39;KernelShap&#39;, &#39;type&#39;: [&#39;blackbox&#39;], &#39;task&#39;: &#39;classification&#39;, &#39;explanations&#39;: [&#39;local&#39;, &#39;global&#39;], &#39;params&#39;: { &#39;groups&#39;: None, &#39;group_names&#39;: None, &#39;weights&#39;: None, &#39;kwargs&#39;: {}, &#39;summarise_background&#39;: False } }) . explanation = explainer.explain(X_test.to_numpy()[0].reshape(1, -1)) . . l1_reg=&#34;auto&#34; is deprecated and in the next version (v0.29) the behavior will change from a conditional use of AIC to simply &#34;num_features(10)&#34;! l1_reg=&#34;auto&#34; is deprecated and in the next version (v0.29) the behavior will change from a conditional use of AIC to simply &#34;num_features(10)&#34;! . shap.force_plot( explanation.expected_value[0], explanation.shap_values[0][0, :] , X_test.to_numpy()[0].reshape(1, -1), X_train.columns, ) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. Tree SHAP . from alibi.explainers import TreeShap . path_dependent_explainer = TreeShap(rf, task=&#39;classification&#39;) . path_dependent_explainer.fit(X_test) . TreeShap(meta={ &#39;name&#39;: &#39;TreeShap&#39;, &#39;type&#39;: [&#39;whitebox&#39;], &#39;task&#39;: &#39;classification&#39;, &#39;explanations&#39;: [&#39;local&#39;, &#39;global&#39;], &#39;params&#39;: { &#39;summarise_background&#39;: False, &#39;algorithm&#39;: &#39;interventional&#39;, &#39;kwargs&#39;: {} } }) . path_dependent_explanation = path_dependent_explainer.explain(X_test) . # identify false negatives misclassified = (np.logical_and(y_test == 1, xgbc_pred == 0)).nonzero()[0] X_misclassified = X_test[misclassified] # explain the predictions path_dependent_shap_values = path_dependent_explanation.shap_values[0] shap_vals_misclassified = path_dependent_shap_values[misclassified, :] . Example-Based Explanation . from alibi.explainers import CounterFactual from time import time . shape = (1,) + X_train.shape[1:] X = X_test.to_numpy()[0].reshape((1,) + X_test.to_numpy()[0].shape) . # initialize explainer cf = CounterFactual(rf.predict_proba, shape, distance_fn=&#39;l1&#39;, target_proba=0.9, target_class=1, max_iter=5000, early_stop=50, lam_init=1e-1, max_lam_steps=10, tol=0.05, learning_rate_init=0.1, feature_range=(-1e10, 1e10), eps=0.01) . explanation = cf.explain(X) . No appropriate lambda range found, try decreasing lam_init Traceback (most recent call last): File &#34;d: anaconda3 envs alibi lib site-packages alibi explainers counterfactual.py&#34;, line 522, in _minimize_loss lb_ix = np.where(cf_count &gt; 0)[0][1] # take the second order of magnitude with some CFs as lower-bound IndexError: index 1 is out of bounds for axis 0 with size 0 . UnboundLocalError Traceback (most recent call last) &lt;ipython-input-463-cdfc4c0a592b&gt; in &lt;module&gt; -&gt; 1 explanation = cf.explain(X) d: anaconda3 envs alibi lib site-packages alibi explainers counterfactual.py in explain(self, X) 338 339 # minimize loss iteratively --&gt; 340 self._minimize_loss(X, X_init, Y) 341 342 return_dict = self.return_dict.copy() d: anaconda3 envs alibi lib site-packages alibi explainers counterfactual.py in _minimize_loss(self, X, X_init, Y) 524 except IndexError: 525 logger.exception(&#39;No appropriate lambda range found, try decreasing lam_init&#39;) --&gt; 526 lam_lb = np.ones(self.batch_size) * lams[lb_ix] 527 528 # find the upper bound UnboundLocalError: local variable &#39;lb_ix&#39; referenced before assignment . Counterfactuals guided by prototypes . from alibi.explainers import CounterFactualProto . # initialize explainer, fit and generate counterfactual cf = CounterFactualProto(rf.predict_proba, shape, use_kdtree=True, theta=10., max_iterations=1000, feature_range=(X_train.min(axis=0), X_train.max(axis=0)), c_init=1., c_steps=10) cf.fit(X_train.to_numpy()) . No encoder specified. Using k-d trees to represent class prototypes. . CounterFactualProto(meta={ &#39;name&#39;: &#39;CounterFactualProto&#39;, &#39;type&#39;: [&#39;blackbox&#39;, &#39;tensorflow&#39;, &#39;keras&#39;], &#39;explanations&#39;: [&#39;local&#39;], &#39;params&#39;: { &#39;write_dir&#39;: None, &#39;update_num_grad&#39;: 1, &#39;clip&#39;: (-1000.0, 1000.0), &#39;eps&#39;: (0.001, 0.001), &#39;c_steps&#39;: 10, &#39;c_init&#39;: 1.0, &#39;max_iterations&#39;: 1000, &#39;learning_rate_init&#39;: 0.01, &#39;use_kdtree&#39;: True, &#39;ohe&#39;: False, &#39;cat_vars&#39;: None, &#39;theta&#39;: 10.0, &#39;gamma&#39;: 0.0, &#39;feature_range&#39;: ( No 2.00 Age 15.00 AgeG1 0.00 cTnITimes 1.00 cTnI 0.01 cTnICKMBOrdinal1 0.00 NTproBNP 20.00 PCT1 0.00 LYM1 0.37 N2L1 0.40 CRP1 0.00 CRP2 0.06 ALB2 28.10 Phlegm 0.00 SoreThroat 0.00 Headache 0.00 Fatigue 0.00 Diarrhea 0.00 NauseaNVomit 0.00 CAD 0.00 dtype: float64, No 97.00 Age 80.00 AgeG1 1.00 cTnITimes 23.00 cTnI 2.60 cTnICKMBOrdinal1 1.00 NTproBNP 8650.00 PCT1 5.75 LYM1 3.37 N2L1 9.38 CRP1 128.31 CRP2 183.56 ALB2 43.90 Phlegm 1.00 SoreThroat 1.00 Headache 1.00 Fatigue 1.00 Diarrhea 1.00 NauseaNVomit 1.00 CAD 1.00 dtype: float64 ), &#39;beta&#39;: 0.1, &#39;kappa&#39;: 0.0, &#39;shape&#39;: (1, 20), &#39;is_model&#39;: False, &#39;is_model_keras&#39;: False, &#39;is_ae&#39;: False, &#39;is_ae_keras&#39;: False, &#39;is_enc&#39;: False, &#39;is_enc_keras&#39;: False, &#39;enc_or_kdtree&#39;: True, &#39;is_cat&#39;: False, &#39;update_feature_range&#39;: True, &#39;center&#39;: True, &#39;smooth&#39;: 1.0, &#39;standardize_cat_vars&#39;: False, &#39;disc_perc&#39;: (25, 50, 75), &#39;w&#39;: None, &#39;d_type&#39;: &#39;abdm&#39;, &#39;trustscore_kwargs&#39;: None } }) . explanation = cf.explain(X) . No counterfactual found! . print(&#39;Original prediction: {}&#39;.format(explanation.orig_class)) print(&#39;Counterfactual prediction: {}&#39;.format(explanation.cf[&#39;class&#39;])) . Original prediction: 0 . TypeError Traceback (most recent call last) &lt;ipython-input-470-a1b473768443&gt; in &lt;module&gt; 1 print(&#39;Original prediction: {}&#39;.format(explanation.orig_class)) -&gt; 2 print(&#39;Counterfactual prediction: {}&#39;.format(explanation.cf[&#39;class&#39;])) TypeError: &#39;NoneType&#39; object is not subscriptable . orig = X * sigma + mu counterfactual = explanation.cf[&#39;X&#39;] * sigma + mu delta = counterfactual - orig for i, f in enumerate(feature_names): if np.abs(delta[0][i]) &gt; 1e-4: print(&#39;{}: {}&#39;.format(f, delta[0][i])) .",
            "url": "https://wuhanstudio.github.io/notebooks/jupyter/2020/06/03/covid.html",
            "relUrl": "/jupyter/2020/06/03/covid.html",
            "date": " • Jun 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Interpretable ML - Linear Regression",
            "content": "import pandas as pd . import numpy as np np.set_printoptions(suppress=True) . Read Dataset . bike_day = pd.read_csv(&quot;dataset/bike/day.csv&quot;) . bike_day.head() . instant dteday season yr mnth holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt . 0 1 | 2011-01-01 | 1 | 0 | 1 | 0 | 6 | 0 | 2 | 0.344167 | 0.363625 | 0.805833 | 0.160446 | 331 | 654 | 985 | . 1 2 | 2011-01-02 | 1 | 0 | 1 | 0 | 0 | 0 | 2 | 0.363478 | 0.353739 | 0.696087 | 0.248539 | 131 | 670 | 801 | . 2 3 | 2011-01-03 | 1 | 0 | 1 | 0 | 1 | 1 | 1 | 0.196364 | 0.189405 | 0.437273 | 0.248309 | 120 | 1229 | 1349 | . 3 4 | 2011-01-04 | 1 | 0 | 1 | 0 | 2 | 1 | 1 | 0.200000 | 0.212122 | 0.590435 | 0.160296 | 108 | 1454 | 1562 | . 4 5 | 2011-01-05 | 1 | 0 | 1 | 0 | 3 | 1 | 1 | 0.226957 | 0.229270 | 0.436957 | 0.186900 | 82 | 1518 | 1600 | . Feature Engineering . Numerical Features . bike_day.temp = bike_day.temp * (39 - (-8)) + (-8) bike_day.atemp = bike_day.atemp * (50 - (16)) + (16) bike_day.windspeed = bike_day.windspeed * 67 bike_day.hum = bike_day.hum * 100 . Datetime . bike_start_day = pd.to_datetime(&#39;2011-01-01&#39;) bike_day[&#39;dteday&#39;] = pd.to_datetime(bike_day[&#39;dteday&#39;]) bike_day[&#39;days_since_2011&#39;] = (bike_day.dteday - bike_start_day).dt.days . selected_features = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;, &quot;days_since_2011&quot;] X = bike_day[selected_features] y = bike_day.cnt . Categorical Features . Year . bike_day.yr[bike_day.loc[:, &#39;yr&#39;] == 0] = &#39;2011&#39; bike_day.yr[bike_day.loc[:, &#39;yr&#39;] == 1] = &#39;2012&#39; . &lt;ipython-input-222-fa42af29cf76&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.yr[bike_day.loc[:, &#39;yr&#39;] == 0] = &#39;2011&#39; . X = X.join(pd.get_dummies(bike_day.yr)) . Season . bike_day.season[bike_day.season == 1] = &#39;SPRING&#39; bike_day.season[bike_day.season == 2] = &#39;SUMMER&#39; bike_day.season[bike_day.season == 3] = &#39;FALL&#39; bike_day.season[bike_day.season == 4] = &#39;WINTER&#39; . &lt;ipython-input-224-6a2d6c4210e2&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.season[bike_day.season == 1] = &#39;SPRING&#39; . X = X.join(pd.get_dummies(bike_day.season)) . Weather . bike_day.weathersit[bike_day.weathersit == 1] = &#39;GOOD&#39; bike_day.weathersit[bike_day.weathersit == 2] = &#39;MISTY&#39; bike_day.weathersit[bike_day.weathersit == 3] = &#39;RAIN/SNOW/STORM&#39; . &lt;ipython-input-226-3f30ca62512e&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.weathersit[bike_day.weathersit == 1] = &#39;GOOD&#39; &lt;ipython-input-226-3f30ca62512e&gt;:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.weathersit[bike_day.weathersit == 2] = &#39;MISTY&#39; &lt;ipython-input-226-3f30ca62512e&gt;:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.weathersit[bike_day.weathersit == 3] = &#39;RAIN/SNOW/STORM&#39; . X = X.join(pd.get_dummies(bike_day.weathersit)) . Month . bike_day.mnth[bike_day.mnth == 1] = &#39;JAN&#39; bike_day.mnth[bike_day.mnth == 2] = &#39;FEB&#39; bike_day.mnth[bike_day.mnth == 3] = &#39;MAR&#39; bike_day.mnth[bike_day.mnth == 4] = &#39;APR&#39; bike_day.mnth[bike_day.mnth == 5] = &#39;MAY&#39; bike_day.mnth[bike_day.mnth == 6] = &#39;JUN&#39; bike_day.mnth[bike_day.mnth == 7] = &#39;JUL&#39; bike_day.mnth[bike_day.mnth == 8] = &#39;AUG&#39; bike_day.mnth[bike_day.mnth == 9] = &#39;SEP&#39; bike_day.mnth[bike_day.mnth == 10] = &#39;OCT&#39; bike_day.mnth[bike_day.mnth == 11] = &#39;NOV&#39; bike_day.mnth[bike_day.mnth == 12] = &#39;DEC&#39; . &lt;ipython-input-228-c8d46e89c532&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy bike_day.mnth[bike_day.mnth == 1] = &#39;JAN&#39; . X = X.join(pd.get_dummies(bike_day.mnth)) . Train test split . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(), train_size = 0.7, random_state = 0) . Training . from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge . Ridge . rr = Ridge() . rr.fit(X_train, y_train) . Ridge() . Lasso . lr = Lasso(normalize=True) . lr.fit(X_train, y_train) . Lasso(normalize=True) . Metrics . from sklearn.metrics import mean_squared_error . Ridge . y_pred_r = rr.predict(X_test) . mean_squared_error(y_test, y_pred_r) . 650885.6073230483 . X.columns . Index([&#39;temp&#39;, &#39;atemp&#39;, &#39;hum&#39;, &#39;windspeed&#39;, &#39;days_since_2011&#39;, &#39;2011&#39;, &#39;2012&#39;, &#39;FALL&#39;, &#39;SPRING&#39;, &#39;SUMMER&#39;, &#39;WINTER&#39;, &#39;GOOD&#39;, &#39;MISTY&#39;, &#39;RAIN/SNOW/STORM&#39;, &#39;APR&#39;, &#39;AUG&#39;, &#39;DEC&#39;, &#39;FEB&#39;, &#39;JAN&#39;, &#39;JUL&#39;, &#39;JUN&#39;, &#39;MAR&#39;, &#39;MAY&#39;, &#39;NOV&#39;, &#39;OCT&#39;, &#39;SEP&#39;], dtype=&#39;object&#39;) . rr.coef_ . array([ 88.56269292, 26.35233461, -20.35055709, -50.15770336, 0.11737953, -931.02767102, 931.02767102, 129.9776705 , -858.75619916, 52.74840848, 676.03012019, 633.50374363, 356.83084875, -990.33459238, 77.61987747, -103.53517043, -229.15948447, -66.61631165, -164.93492354, -531.25800384, -128.86883146, 332.58997771, 377.06743755, -389.4554859 , 245.35738634, 581.19353222]) . rr.intercept_ . 3670.8640712920055 . Lasso . y_pred_l = lr.predict(X_test) . mean_squared_error(y_test, y_pred_l) . 659705.342130416 . lr.coef_ . array([ 75.4720349 , 33.18166704, -16.06025245, -43.72312824, 0. , -1891.66068971, 0. , -0. , -1045.36798995, 0. , 240.19341828, 283.91219444, -0. , -1414.16952065, 0. , 0. , -0. , -0. , -116.21917705, -301.81734998, -0. , 251.78401898, 273.0091002 , -13.95883721, 504.19655444, 651.96621325]) . lr.intercept_ . 4739.781565530179 . Plot . import altair as alt . lr_pred_df = pd.DataFrame({&#39;day&#39;: X_test[:, 4], &#39;y_test&#39;: y_test, &#39;y_pred&#39;: y_pred_l, }) . lr_pred_df = lr_pred_df.melt(&#39;day&#39;, var_name=&#39;data&#39;, value_name=&#39;value&#39;) . scales = alt.selection_interval(bind=&#39;scales&#39;) alt.Chart(lr_pred_df).mark_line(point=True).encode( x = &#39;day:Q&#39;, y = &#39;value:Q&#39;, color=&#39;data:N&#39;, tooltip=[ &#39;data:N&#39;, &#39;value:N&#39;] ).add_selection( scales ).properties(width=800) .",
            "url": "https://wuhanstudio.github.io/notebooks/jupyter/2020/05/27/iml-lr.html",
            "relUrl": "/jupyter/2020/05/27/iml-lr.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://wuhanstudio.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://wuhanstudio.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://wuhanstudio.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}